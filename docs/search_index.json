[
["streaming.html", "Chapter 9 Streaming 9.1 Overview 9.2 Transformations 9.3 Shiny", " Chapter 9 Streaming 9.1 Overview One can understand a stream as an unbound data frame, meaning, a data frame with finite columns but infinite rows. Streams are most relevant when processing real time data; for example, when analyzing a Twitter feed or stock prices. Both examples have well defined columns, like ‘tweet’ or ‘price’, but there are always new rows of data to be analyzed. Spark provided initial support for streams with Spark’s DStreams; however, a more versatile and efficient replacement is available through Spark structured streams. Structured streams provide scalable and fault-torerant data processing over streams of data. Meaning that, one can use many machines to process multiple streaming sources, perform joins with other streams or static sources, and recover from failures with at-least-once guarantees (where each message is certain to be delivered, but may do so multiple times). In order to use structured streams in sparklyr, one needs to define the sources, transformations and a destination: The sources are defined using any of the stream_read_*() functions to read streams of data from various data sources. The transformations can be specified using dplyr, SQL, scoring pipelines or R code through spark_apply(). The destination is defined with the stream_write_*() functions, it often also referenced as a sink. Since the transformation step is optional, the simplest stream we can define is to continuously process files, which would effectively copy text files between source and destination. We can define this copy-stream in sparklyr as follows: library(sparklyr) sc &lt;- spark_connect(master = &quot;local&quot;) stream &lt;- stream_read_text(sc, &quot;source/&quot;) %&gt;% stream_write_text(&quot;destination/&quot;) Once executed, the stream will monitor the source path and process data into the destination/ path as it arrives. We can use view_stream() to track the rows per second (rps) being processed in the source, destination and their latest values over time: stream_view(stream) FIGURE 9.1: Viewing a Spark Stream with sparklyr Notice that the rows-per-second in the destination stream are higher than the rows-per-second in the source stream; this is expected and desireable since Spark measures incoming rates from the source, but actual row processing times in the destination stream. Use stream_stop() to properly stop processing data from this stream: stream_stop(stream) In order to reproduce the above example, one needs to feed streaming data into the source/ path. This was accomplished by running, before stream_view(stream), the following script to produce a file every second containing lines of text that follow two overlapping binomial distributions. In practice, you would connect to existing sources without having to generate data artificially. unlink(c(&quot;source/&quot;, &quot;destination/&quot;), recursive = TRUE) dir.create(&quot;source&quot;) callr::r_bg(function() { dist &lt;- floor(10 + 1e5 * (dbinom(1:50, 50, 0.7) + dbinom(1:50, 50, 0.3))) for (i in seq_len(10)) { writeLines(paste(&quot;Row&quot;, 1:dist[i]), paste0(&quot;source/hello_&quot;, i, &quot;.txt&quot;)) Sys.sleep(1) } }) For the subsequent examples, a stream with one hundred rows of text will be used: writeLines(paste(&quot;Row&quot;, 1:100), &quot;source/rows.txt&quot;) 9.2 Transformations Streams can be transformed using dplyr, SQL, pipelines or R code. We can use as many transformations as needed in the same way that Spark data frames can be transformed with sparklyr. The transformation source can be streams or data frames but the output is always a stream. If needed, one can always take a snapshot from the destination stream and save the output as a data frame, which is what sparklyr will do for you if a destination stream is not specified. Conceptually, this looks as follows: FIGURE 9.2: Streams Transformation Diagram 9.2.1 dplyr Using dplyr, we can process each row of the stream; for example, we can filter the stream to only the rows containing a number one: library(dplyr, warn.conflicts = FALSE) stream_read_text(sc, &quot;source/&quot;) %&gt;% filter(text %like% &quot;%1%&quot;) ## # Source: lazy query [?? x 1] ## # Database: spark_connection ## text ## &lt;chr&gt; ## 1 Row 1 ## 2 Row 10 ## 3 Row 11 ## 4 Row 12 ## 5 Row 13 ## 6 Row 14 ## 7 Row 15 ## 8 Row 16 ## 9 Row 17 ## 10 Row 18 ## # ... with more rows Since the destination was not specified, sparklyr creates a temporary memory stream and previews the contents of a stream by capturing a few seconds of streaming data. We can also aggregate data with dplyr, stream_read_text(sc, &quot;source/&quot;) %&gt;% summarise(n = n()) ## # Source: lazy query [?? x 1] ## # Database: spark_connection ## n ## &lt;dbl&gt; ## 1 100 and even join across many concurrent streams: left_join( stream_read_text(sc, &quot;source/&quot;) %&gt;% stream_watermark(), stream_read_text(sc, &quot;source/&quot;) %&gt;% stream_watermark() %&gt;% mutate(random = rand()), ) ## Joining, by = c(&quot;text&quot;, &quot;timestamp&quot;) ## # Source: lazy query [?? x 3] ## # Database: spark_connection ## text timestamp random ## &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; ## 1 Row 5 2018-07-11 07:55:20 0.0198 ## 2 Row 7 2018-07-11 07:55:20 0.997 ## 3 Row 13 2018-07-11 07:55:20 0.0134 ## 4 Row 24 2018-07-11 07:55:20 0.740 ## 5 Row 25 2018-07-11 07:55:20 0.567 ## 6 Row 32 2018-07-11 07:55:20 0.937 ## 7 Row 52 2018-07-11 07:55:20 0.291 ## 8 Row 64 2018-07-11 07:55:20 0.693 ## 9 Row 77 2018-07-11 07:55:20 0.350 ## 10 Row 84 2018-07-11 07:55:20 0.367 ## # ... with more rows However, some operations, require watermarks to define when to stop waiting for late data. You can specify watermarks in sparklyr using stream_watermak(), see also handling late data in Spark’s documentation. 9.2.2 Pipelines Spark pipelines can be used for scoring streams, but not to train over streaming data. The former is fully supported while the latter is a feature under active development by the Spark community. To use a pipeline for scoring a stream, first train a Spark pipeline over a static dataset. Once trained, save the pipeline, then reload and score over a stream as follows: fitted_pipeline &lt;- ml_load(sc, &quot;iris-fitted/&quot;) stream_read_csv(sc, &quot;iris-in&quot;) %&gt;% sdf_transform(fitted_pipeline) %&gt;% stream_write_csv(&quot;iris-out&quot;) 9.2.3 R Code Arbitrary R code can also be used to transform a stream with the use of spark_apply(). Following the same principles from executing R code over Spark data frames, for structured streams, spark_apply() runs R code over each executor in the cluster where data is available, this enables processing high-throughput streams and fullfill low-latency requirements. The following example splits a stream of Row # line entries and adds jitter using R code: stream_read_text(sc, &quot;source/&quot;) %&gt;% spark_apply(function(df) { data.frame( data = jitter(as.numeric(gsub(&quot;Row &quot;, &quot;&quot;, df$text))) ) }) ## # Source: table&lt;sparklyr_tmp_144943c74e843&gt; [?? x 1] ## # Database: spark_connection ## data ## &lt;dbl&gt; ## 1 0.901 ## 2 2.16 ## 3 3.06 ## 4 3.84 ## 5 4.89 ## 6 5.80 ## 7 6.81 ## 8 8.13 ## 9 9.05 ## 10 10.1 ## # ... with more rows 9.3 Shiny Streams can be used with Shiny by making use of the reactiveSpark() to retrieve the stream as a reactive data source. Internally, reactiveSpark() makes use of reactivePoll() to check the stream’s timestamp and collect the stream contents when needed. The following Shiny application makes use of reactiveSpark() to view a Spark stream summarized with dplyr: library(shiny) library(sparklyr) library(dplyr) sc &lt;- spark_connect(master = &quot;local&quot;) ui &lt;- fluidPage( sidebarLayout( mainPanel( tableOutput(&quot;table&quot;) ) ) ) server &lt;- function(input, output, session) { pollData &lt;- stream_read_text(sc, &quot;source/&quot;) %&gt;% summarise(n = n()) %&gt;% reactiveSpark(session = session) output$table &lt;- renderTable({ pollData() }) } shinyApp(ui = ui, server = server) "]
]
