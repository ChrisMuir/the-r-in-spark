[
["streaming.html", "Chapter 9 Streaming 9.1 Overview 9.2 Transformations 9.3 Shiny", " Chapter 9 Streaming 9.1 Overview One can understand a stream as an unbound data frame, meaning, a data frame with finite columns but infinite rows. Streams are most relevant when processing real time data; for example, when analyzing a Twitter feed or stock prices. Both examples have well defined columns, like ‘tweet’ or ‘price’, but there are always new rows of data to be analyzed. Spark provided initial support for streams with Spark’s DStreams; however, a more versatile and efficient replacement is available through Spark structured streams. Structured streams provide scalable and fault-torerant data processing over streams of data. Meaning that, one can use many machines to process multiple streaming sources, perform joins with other streams or static sources, and recover from failures with at-least-once guarantees (where each message is certain to be delivered, but may do so multiple times). In order to use structured streams in sparklyr, one needs to define the sources, transformations and destination: The sources are defined using any of the stream_read_*() functions to read streams of data from various data sources. The transformations can be specified using dplyr, SQL, scoring pipelines or R code through spark_apply(). The destination is defined with the stream_write_*() functions, it often also referenced as a sink. Since the transformation step is optional, the simplest stream we can define is to continuously process files, which would effectively copy text files between source and destination. We can define this copy-stream in sparklyr as follows: library(sparklyr) sc &lt;- spark_connect(master = &quot;local&quot;) stream &lt;- stream_read_text(sc, &quot;source/&quot;) %&gt;% stream_write_text(&quot;destination/&quot;) Once executed, the stream will monitor the source path and process data into the destination/ path as it arrives. We can use view_stream() to track the rows per second (rps) being processed in the source, destination and their latest values over time: stream_view(stream) FIGURE 9.1: Viewing a Spark Stream with sparklyr For the stream sources, rps tracks the actual number of rows being processed, since there is no objective way of measuring how fast the source can deliver rows appart from counting the rows a streaming sources provides. However, when writting rows into a streaming destination, we Spark can measure how fast rows were actually processed and therefore, in a healthy systems, the destination rps should be higher than the source to keep up with incoming data. Use stream_stop() to properly stop processing data from this stream: stream_stop(stream) Notice that, in order to reproduce the above example, one needs to feed streaming data into the source/ path. This was accomplished by running, before stream_view(stream), the following script to produce a file every second containing lines of text that follow two overlapping binomial distribution. In practice, you would connect to existing sources without having to generate data artificially. unlink(c(&quot;source/&quot;, &quot;destination/&quot;), recursive = TRUE) dir.create(&quot;source&quot;) callr::r_bg(function() { dist &lt;- floor(10 + 1e5 * (dbinom(1:50, 50, 0.7) + dbinom(1:50, 50, 0.3))) for (i in seq_len(10)) { writeLines(paste(&quot;Row&quot;, 1:dist[i]), paste0(&quot;source/hello_&quot;, i, &quot;.txt&quot;)) Sys.sleep(1) } }) For the subsequent examples, a stream processing one hundred rows of data will be used: writeLines(paste(&quot;Row&quot;, 1:100), &quot;source/rows.txt&quot;) 9.2 Transformations Streams can be transformed using dplyr, SQL, scoring pipelines or R code through spark_apply(). We can use as many transformations as needed in the same way that Spark data frame can be transformed with sparklyr. The transformation source can be streams or data frames but the output is always a stream. If needed, one can always take a snapshot from the destination stream and save the output as a data frame, but even in this case, the destination stream must be defined. Conceptually, this looks as follows: FIGURE 9.2: Stream Transformations Diagram 9.2.1 dplyr Using dplyr, we can process each row of the stream; for example, we can filter the stream to only the rows containing a number one: library(dplyr, warn.conflicts = FALSE) stream_read_text(sc, &quot;source/&quot;) %&gt;% filter(text %like% &quot;%1%&quot;) ## # Source: lazy query [?? x 1] ## # Database: spark_connection ## text ## &lt;chr&gt; ## 1 Row 1 ## 2 Row 10 ## 3 Row 11 ## 4 Row 12 ## 5 Row 13 ## 6 Row 14 ## 7 Row 15 ## 8 Row 16 ## 9 Row 17 ## 10 Row 18 ## # ... with more rows Notice that in the example above, the destination was not specified; when this happens, sparklyr creates a temporary memory stream and previews the contents of a stream by capturing a few seconds of streaming data. We can also aggregate data with dplyr, stream_read_text(sc, &quot;source/&quot;) %&gt;% summarise(n = n()) ## # Source: lazy query [?? x 1] ## # Database: spark_connection ## n ## &lt;dbl&gt; ## 1 100 and even join across many concurrent streams: left_join( stream_read_text(sc, &quot;source/&quot;) %&gt;% stream_watermark(), stream_read_text(sc, &quot;source/&quot;) %&gt;% stream_watermark() %&gt;% mutate(random = rand()), ) ## Joining, by = c(&quot;text&quot;, &quot;timestamp&quot;) ## # Source: lazy query [?? x 3] ## # Database: spark_connection ## text timestamp random ## &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; ## 1 Row 9 2018-07-11 05:57:08 0.546 ## 2 Row 25 2018-07-11 05:57:08 0.248 ## 3 Row 29 2018-07-11 05:57:08 0.0539 ## 4 Row 48 2018-07-11 05:57:08 0.808 ## 5 Row 54 2018-07-11 05:57:08 0.208 ## 6 Row 58 2018-07-11 05:57:08 0.671 ## 7 Row 65 2018-07-11 05:57:08 0.398 ## 8 Row 72 2018-07-11 05:57:08 0.740 ## 9 Row 73 2018-07-11 05:57:08 0.689 ## 10 Row 74 2018-07-11 05:57:08 0.254 ## # ... with more rows Notice that some operations, like joins, require watermarks to define late data and specify when to stop waiting for it. You can specify watermarks in sparklyr using stream_watermak(), see also handling late data in Spark’s documentation. 9.2.2 Pipelines Spark pipelines can be used for scoring streams; however, not to train over streaming data. The former is fully supperted while the latter is a feature under acteve development by the Spark community. To use a pipeline for scoring a stream, first train a Spark pipeline over a static dataset. Once trained, save the pipeline, then reload and score over a stream as follows: fitted_pipeline &lt;- ml_load(sc, &quot;iris-fitted/&quot;) stream_read_csv(sc, &quot;iris-in&quot;) %&gt;% sdf_transform(fitted_pipeline) %&gt;% stream_write_csv(&quot;iris-out&quot;) 9.2.3 R Code Arbitrary R code can also be used to transform a stream with the use of spark_apply(). Just like processing data over a Spark dataframe, spark_apply() runs over each executor in the cluster where data is partitioned to parallelize over wide streams and resuce latency. The following example splits a stream of Row # line entries and adds jitter using R code: stream_read_text(sc, &quot;source/&quot;) %&gt;% spark_apply(function(df) { data.frame( rows = jitter( as.numeric( gsub(&quot;Row &quot;, &quot;&quot;, df$text) ) ) ) }) ## # Source: table&lt;sparklyr_tmp_ba5e4cf1afc4&gt; [?? x 1] ## # Database: spark_connection ## rows ## &lt;dbl&gt; ## 1 0.809 ## 2 1.93 ## 3 3.08 ## 4 4.20 ## 5 4.82 ## 6 5.87 ## 7 7.05 ## 8 7.98 ## 9 8.90 ## 10 9.95 ## # ... with more rows 9.3 Shiny Streams can be used with Shiny by making use of the reactiveSpark() to retrieve the stream as a reactive data source. Internally, reactiveSpark() makes use of reactivePoll() to check the stream’s timestamp and collect the stream contents when needed. A simple Shiny application using a reactiveSpark() source looks like: library(shiny) library(sparklyr) library(dplyr) sc &lt;- spark_connect(master = &quot;local&quot;) ui &lt;- fluidPage( sidebarLayout( mainPanel( tableOutput(&quot;table&quot;) ) ) ) server &lt;- function(input, output, session) { pollData &lt;- stream_read_text(sc, &quot;source/&quot;) %&gt;% summarise(n = n()) %&gt;% reactiveSpark(session = session) output$table &lt;- renderTable({ pollData() }) } shinyApp(ui = ui, server = server) "]
]
