[
["streaming.html", "Chapter 9 Streaming 9.1 Overview 9.2 Transformations 9.3 Shiny", " Chapter 9 Streaming 9.1 Overview One can understand a stream as an unbound data frame, meaning, a data frame with finite columns but infinite rows. Streams are most relevant when processing real time data; for example, when analyzing a Twitter feed or stock prices. Both examples have well defined columns, like ‘tweet’ or ‘price’, but there are always new rows of data to be analyzed. Spark provided initial support for streams with Spark’s DStreams; however, a more versatile and efficient replacement is available through Spark structured streams. Structured streams provide scalable and fault-torerant data processing over streams of data. Meaning that, one can use many machines to process multiple streaming sources, perform joins with other streams or static sources, and recover from failures with at-least-once guarantees (where each message is certain to be delivered, but may do so multiple times). In order to use structured streams in sparklyr, one needs to define the sources, transformations and a destination: The sources are defined using any of the stream_read_*() functions to read streams of data from various data sources. The transformations can be specified using dplyr, SQL, scoring pipelines or R code through spark_apply(). The destination is defined with the stream_write_*() functions, it often also referenced as a sink. Since the transformation step is optional, the simplest stream we can define is to continuously process files, which would effectively copy text files between source and destination. We can define this copy-stream in sparklyr as follows: library(sparklyr) sc &lt;- spark_connect(master = &quot;local&quot;) stream &lt;- stream_read_text(sc, &quot;source/&quot;) %&gt;% stream_write_text(&quot;destination/&quot;) Once executed, the stream will monitor the source path and process data into the destination/ path as it arrives. We can use view_stream() to track the rows per second (rps) being processed in the source, destination and their latest values over time: stream_view(stream) FIGURE 9.1: Viewing a Spark Stream with sparklyr Notice that the rows-per-second in the destination stream are higher than the rows-per-second in the source stream; this is expected and desireable since Spark measures incoming rates from the source, but actual row processing times in the destination stream. Use stream_stop() to properly stop processing data from this stream: stream_stop(stream) Notice that, in order to reproduce the above example, one needs to feed streaming data into the source/ path. This was accomplished by running, before stream_view(stream), the following script to produce a file every second containing lines of text that follow two overlapping binomial distributions. In practice, you would connect to existing sources without having to generate data artificially. unlink(c(&quot;source/&quot;, &quot;destination/&quot;), recursive = TRUE) dir.create(&quot;source&quot;) callr::r_bg(function() { dist &lt;- floor(10 + 1e5 * (dbinom(1:50, 50, 0.7) + dbinom(1:50, 50, 0.3))) for (i in seq_len(10)) { writeLines(paste(&quot;Row&quot;, 1:dist[i]), paste0(&quot;source/hello_&quot;, i, &quot;.txt&quot;)) Sys.sleep(1) } }) For the subsequent examples, a stream with one hundred rows of text will be used: writeLines(paste(&quot;Row&quot;, 1:100), &quot;source/rows.txt&quot;) 9.2 Transformations Streams can be transformed using dplyr, SQL, pipelines or R code. We can use as many transformations as needed in the same way that Spark data frames can be transformed with sparklyr. The transformation source can be streams or data frames but the output is always a stream. If needed, one can always take a snapshot from the destination stream and save the output as a data frame, which is what sparklyr will do for you if a destination stream is not specified. Conceptually, this looks as follows: FIGURE 9.2: Streams Transformation Diagram 9.2.1 dplyr Using dplyr, we can process each row of the stream; for example, we can filter the stream to only the rows containing a number one: library(dplyr, warn.conflicts = FALSE) stream_read_text(sc, &quot;source/&quot;) %&gt;% filter(text %like% &quot;%1%&quot;) ## # Source: lazy query [?? x 1] ## # Database: spark_connection ## text ## &lt;chr&gt; ## 1 Row 1 ## 2 Row 10 ## 3 Row 11 ## 4 Row 12 ## 5 Row 13 ## 6 Row 14 ## 7 Row 15 ## 8 Row 16 ## 9 Row 17 ## 10 Row 18 ## # ... with more rows Since the destination was not specified, sparklyr creates a temporary memory stream and previews the contents of a stream by capturing a few seconds of streaming data. We can also aggregate data with dplyr, stream_read_text(sc, &quot;source/&quot;) %&gt;% summarise(n = n()) ## # Source: lazy query [?? x 1] ## # Database: spark_connection ## n ## &lt;dbl&gt; ## 1 100 and even join across many concurrent streams: left_join( stream_read_text(sc, &quot;source/&quot;) %&gt;% stream_watermark(), stream_read_text(sc, &quot;source/&quot;) %&gt;% stream_watermark() %&gt;% mutate(random = rand()), ) ## Joining, by = c(&quot;text&quot;, &quot;timestamp&quot;) ## # Source: lazy query [?? x 3] ## # Database: spark_connection ## text timestamp random ## &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; ## 1 Row 6 2018-07-11 07:31:54 0.197 ## 2 Row 12 2018-07-11 07:31:54 0.375 ## 3 Row 18 2018-07-11 07:31:54 0.552 ## 4 Row 22 2018-07-11 07:31:54 0.389 ## 5 Row 63 2018-07-11 07:31:54 0.0596 ## 6 Row 73 2018-07-11 07:31:54 0.929 ## 7 Row 79 2018-07-11 07:31:54 0.483 ## 8 Row 92 2018-07-11 07:31:54 0.635 ## 9 Row 93 2018-07-11 07:31:54 0.341 ## 10 Row 95 2018-07-11 07:31:54 0.611 ## # ... with more rows Notice that some operations, like joins, require watermarks to define late data and specify when to stop waiting for it. You can specify watermarks in sparklyr using stream_watermak(), see also handling late data in Spark’s documentation. 9.2.2 Pipelines Spark pipelines can be used for scoring streams; however, not to train over streaming data. The former is fully supperted while the latter is a feature under acteve development by the Spark community. To use a pipeline for scoring a stream, first train a Spark pipeline over a static dataset. Once trained, save the pipeline, then reload and score over a stream as follows: fitted_pipeline &lt;- ml_load(sc, &quot;iris-fitted/&quot;) stream_read_csv(sc, &quot;iris-in&quot;) %&gt;% sdf_transform(fitted_pipeline) %&gt;% stream_write_csv(&quot;iris-out&quot;) 9.2.3 R Code Arbitrary R code can also be used to transform a stream with the use of spark_apply(). Just like processing data over a Spark data frame, spark_apply() runs over each executor in the cluster where data is partitioned to parallelize over wide streams or reduce latency. The following example splits a stream of Row # line entries and adds jitter using R code: stream_read_text(sc, &quot;source/&quot;) %&gt;% spark_apply(function(df) { data.frame( rows = jitter( as.numeric( gsub(&quot;Row &quot;, &quot;&quot;, df$text) ) ) ) }) ## # Source: table&lt;sparklyr_tmp_126555e4215e1&gt; [?? x 1] ## # Database: spark_connection ## rows ## &lt;dbl&gt; ## 1 1.13 ## 2 2.15 ## 3 3.07 ## 4 4.02 ## 5 5.14 ## 6 5.92 ## 7 7.11 ## 8 8.15 ## 9 8.88 ## 10 10.1 ## # ... with more rows 9.3 Shiny Streams can be used with Shiny by making use of the reactiveSpark() to retrieve the stream as a reactive data source. Internally, reactiveSpark() makes use of reactivePoll() to check the stream’s timestamp and collect the stream contents when needed. A simple Shiny application using a reactiveSpark() source looks like: library(shiny) library(sparklyr) library(dplyr) sc &lt;- spark_connect(master = &quot;local&quot;) ui &lt;- fluidPage( sidebarLayout( mainPanel( tableOutput(&quot;table&quot;) ) ) ) server &lt;- function(input, output, session) { pollData &lt;- stream_read_text(sc, &quot;source/&quot;) %&gt;% summarise(n = n()) %&gt;% reactiveSpark(session = session) output$table &lt;- renderTable({ pollData() }) } shinyApp(ui = ui, server = server) "]
]
