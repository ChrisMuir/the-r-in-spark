*** getSparkContext ***
1: spark_connect(master = "local", config = config)
2: initialize_connection(scon)
3: initialize_connection.spark_shell_connection(scon)
4: tryCatch({
    backend <- invoke_static(sc, "sparklyr.Shell", "getBackend")
    sc$spark_context <- invoke(backend, "getSparkContext")
    if (is.null(sc$spark_context)) {
        conf <- invoke_new(sc, "org.apache.spark.SparkConf")
5: tryCatchList(expr, classes, parentenv, handlers)
6: tryCatchOne(expr, names, parentenv, handlers[[1L]])
7: doTryCatch(return(expr), name, parentenv, handler)
8: invoke(backend, "getSparkContext")
*** setAppName ***
1: spark_connect(master = "local", config = config)
2: initialize_connection(scon)
3: initialize_connection.spark_shell_connection(scon)
4: tryCatch({
    backend <- invoke_static(sc, "sparklyr.Shell", "getBackend")
    sc$spark_context <- invoke(backend, "getSparkContext")
    if (is.null(sc$spark_context)) {
        conf <- invoke_new(sc, "org.apache.spark.SparkConf")
5: tryCatchList(expr, classes, parentenv, handlers)
6: tryCatchOne(expr, names, parentenv, handlers[[1L]])
7: doTryCatch(return(expr), name, parentenv, handler)
8: invoke(conf, "setAppName", sc$app_name)
*** setMaster ***
1: spark_connect(master = "local", config = config)
2: initialize_connection(scon)
3: initialize_connection.spark_shell_connection(scon)
4: tryCatch({
    backend <- invoke_static(sc, "sparklyr.Shell", "getBackend")
    sc$spark_context <- invoke(backend, "getSparkContext")
    if (is.null(sc$spark_context)) {
        conf <- invoke_new(sc, "org.apache.spark.SparkConf")
5: tryCatchList(expr, classes, parentenv, handlers)
6: tryCatchOne(expr, names, parentenv, handlers[[1L]])
7: doTryCatch(return(expr), name, parentenv, handler)
8: invoke(conf, "setMaster", sc$master)
*** setSparkHome ***
1: spark_connect(master = "local", config = config)
2: initialize_connection(scon)
3: initialize_connection.spark_shell_connection(scon)
4: tryCatch({
    backend <- invoke_static(sc, "sparklyr.Shell", "getBackend")
    sc$spark_context <- invoke(backend, "getSparkContext")
    if (is.null(sc$spark_context)) {
        conf <- invoke_new(sc, "org.apache.spark.SparkConf")
5: tryCatchList(expr, classes, parentenv, handlers)
6: tryCatchOne(expr, names, parentenv, handlers[[1L]])
7: doTryCatch(return(expr), name, parentenv, handler)
8: invoke(conf, "setSparkHome", sc$spark_home)
*** set ***
1: spark_connect(master = "local", config = config)
2: initialize_connection(scon)
3: initialize_connection.spark_shell_connection(scon)
4: tryCatch({
    backend <- invoke_static(sc, "sparklyr.Shell", "getBackend")
    sc$spark_context <- invoke(backend, "getSparkContext")
    if (is.null(sc$spark_context)) {
        conf <- invoke_new(sc, "org.apache.spark.SparkConf")
5: tryCatchList(expr, classes, parentenv, handlers)
6: tryCatchOne(expr, names, parentenv, handlers[[1L]])
7: doTryCatch(return(expr), name, parentenv, handler)
8: apply_config(context_config, conf, "set", "spark.")
9: lapply(names(params), function(paramName) {
    configValue <- params[[paramName]]
    if (is.logical(configValue)) {
        configValue <- if (configValue) 
            "true"
10: FUN(X[[i]], ...)
11: invoke(object, method, paste0(prefix, paramName), configValue)
*** set ***
1: spark_connect(master = "local", config = config)
2: initialize_connection(scon)
3: initialize_connection.spark_shell_connection(scon)
4: tryCatch({
    backend <- invoke_static(sc, "sparklyr.Shell", "getBackend")
    sc$spark_context <- invoke(backend, "getSparkContext")
    if (is.null(sc$spark_context)) {
        conf <- invoke_new(sc, "org.apache.spark.SparkConf")
5: tryCatchList(expr, classes, parentenv, handlers)
6: tryCatchOne(expr, names, parentenv, handlers[[1L]])
7: doTryCatch(return(expr), name, parentenv, handler)
8: apply_config(default_config, conf, "set", "spark.")
9: lapply(names(params), function(paramName) {
    configValue <- params[[paramName]]
    if (is.logical(configValue)) {
        configValue <- if (configValue) 
            "true"
10: FUN(X[[i]], ...)
11: invoke(object, method, paste0(prefix, paramName), configValue)
*** setSparkContext ***
1: spark_connect(master = "local", config = config)
2: initialize_connection(scon)
3: initialize_connection.spark_shell_connection(scon)
4: tryCatch({
    backend <- invoke_static(sc, "sparklyr.Shell", "getBackend")
    sc$spark_context <- invoke(backend, "getSparkContext")
    if (is.null(sc$spark_context)) {
        conf <- invoke_new(sc, "org.apache.spark.SparkConf")
5: tryCatchList(expr, classes, parentenv, handlers)
6: tryCatchOne(expr, names, parentenv, handlers[[1L]])
7: doTryCatch(return(expr), name, parentenv, handler)
8: invoke(backend, "setSparkContext", sc$spark_context)
*** sql ***
1: (function (type, host, ...) 
{
    connection <- .rs.findActiveConnection(type, host)
    if (!is.null(connection)) 
        connection$listObjects(...)
2: connection$listObjects(...)
3: connection_list_tables(scon, includeType = TRUE)
4: sort(dbListTables(sc))
5: dbListTables(sc)
6: dbListTables(sc)
7: .local(conn, ...)
8: df_from_sql(conn, "SHOW TABLES")
9: invoke(hive_context(sc), "sql", as.character(sql))
*** version ***
1: (function (type, host, ...) 
{
    connection <- .rs.findActiveConnection(type, host)
    if (!is.null(connection)) 
        connection$listObjects(...)
2: connection$listObjects(...)
3: connection_list_tables(scon, includeType = TRUE)
4: sort(dbListTables(sc))
5: dbListTables(sc)
6: dbListTables(sc)
7: .local(conn, ...)
8: df_from_sql(conn, "SHOW TABLES")
9: invoke(hive_context(sc), "sql", as.character(sql))
10: hive_context(sc)
11: hive_context.spark_shell_connection(sc)
12: create_hive_context(sc)
13: create_hive_context.spark_shell_connection(sc)
14: spark_version(sc)
15: invoke(spark_context(sc), "version")
*** enableHiveSupport ***
1: (function (type, host, ...) 
{
    connection <- .rs.findActiveConnection(type, host)
    if (!is.null(connection)) 
        connection$listObjects(...)
2: connection$listObjects(...)
3: connection_list_tables(scon, includeType = TRUE)
4: sort(dbListTables(sc))
5: dbListTables(sc)
6: dbListTables(sc)
7: .local(conn, ...)
8: df_from_sql(conn, "SHOW TABLES")
9: invoke(hive_context(sc), "sql", as.character(sql))
10: hive_context(sc)
11: hive_context.spark_shell_connection(sc)
12: create_hive_context(sc)
13: create_hive_context.spark_shell_connection(sc)
14: create_hive_context_v2(sc)
15: invoke(builder, "enableHiveSupport")
*** getOrCreate ***
1: (function (type, host, ...) 
{
    connection <- .rs.findActiveConnection(type, host)
    if (!is.null(connection)) 
        connection$listObjects(...)
2: connection$listObjects(...)
3: connection_list_tables(scon, includeType = TRUE)
4: sort(dbListTables(sc))
5: dbListTables(sc)
6: dbListTables(sc)
7: .local(conn, ...)
8: df_from_sql(conn, "SHOW TABLES")
9: invoke(hive_context(sc), "sql", as.character(sql))
10: hive_context(sc)
11: hive_context.spark_shell_connection(sc)
12: create_hive_context(sc)
13: create_hive_context.spark_shell_connection(sc)
14: create_hive_context_v2(sc)
15: invoke(builder, "getOrCreate")
*** conf ***
1: (function (type, host, ...) 
{
    connection <- .rs.findActiveConnection(type, host)
    if (!is.null(connection)) 
        connection$listObjects(...)
2: connection$listObjects(...)
3: connection_list_tables(scon, includeType = TRUE)
4: sort(dbListTables(sc))
5: dbListTables(sc)
6: dbListTables(sc)
7: .local(conn, ...)
8: df_from_sql(conn, "SHOW TABLES")
9: invoke(hive_context(sc), "sql", as.character(sql))
10: hive_context(sc)
11: hive_context.spark_shell_connection(sc)
12: create_hive_context(sc)
13: create_hive_context.spark_shell_connection(sc)
14: create_hive_context_v2(sc)
15: invoke(session, "conf")
*** set ***
1: (function (type, host, ...) 
{
    connection <- .rs.findActiveConnection(type, host)
    if (!is.null(connection)) 
        connection$listObjects(...)
2: connection$listObjects(...)
3: connection_list_tables(scon, includeType = TRUE)
4: sort(dbListTables(sc))
5: dbListTables(sc)
6: dbListTables(sc)
7: .local(conn, ...)
8: df_from_sql(conn, "SHOW TABLES")
9: invoke(hive_context(sc), "sql", as.character(sql))
10: hive_context(sc)
11: hive_context.spark_shell_connection(sc)
12: create_hive_context(sc)
13: create_hive_context.spark_shell_connection(sc)
14: create_hive_context_v2(sc)
15: apply_config(params, conf, "set", "spark.sql.")
16: lapply(names(params), function(paramName) {
    configValue <- params[[paramName]]
    if (is.logical(configValue)) {
        configValue <- if (configValue) 
            "true"
17: FUN(X[[i]], ...)
18: invoke(object, method, paste0(prefix, paramName), configValue)
*** version ***
1: (function (type, host, ...) 
{
    connection <- .rs.findActiveConnection(type, host)
    if (!is.null(connection)) 
        connection$listObjects(...)
2: connection$listObjects(...)
3: connection_list_tables(scon, includeType = TRUE)
4: sort(dbListTables(sc))
5: dbListTables(sc)
6: dbListTables(sc)
7: .local(conn, ...)
8: df_from_sql(conn, "SHOW TABLES")
9: df_from_sdf(sc, sdf)
10: sdf_collect(sdf)
11: sdf_is_streaming(object)
12: spark_version(sc)
13: invoke(spark_context(sc), "version")
*** isStreaming ***
1: (function (type, host, ...) 
{
    connection <- .rs.findActiveConnection(type, host)
    if (!is.null(connection)) 
        connection$listObjects(...)
2: connection$listObjects(...)
3: connection_list_tables(scon, includeType = TRUE)
4: sort(dbListTables(sc))
5: dbListTables(sc)
6: dbListTables(sc)
7: .local(conn, ...)
8: df_from_sql(conn, "SHOW TABLES")
9: df_from_sdf(sc, sdf)
10: sdf_collect(sdf)
11: sdf_is_streaming(object)
12: invoke(x, "isStreaming")
*** version ***
1: (function (type, host, ...) 
{
    connection <- .rs.findActiveConnection(type, host)
    if (!is.null(connection)) 
        connection$listObjects(...)
2: connection$listObjects(...)
3: connection_list_tables(scon, includeType = TRUE)
4: sort(dbListTables(sc))
5: dbListTables(sc)
6: dbListTables(sc)
7: .local(conn, ...)
8: df_from_sql(conn, "SHOW TABLES")
9: df_from_sdf(sc, sdf)
10: sdf_collect(sdf)
11: sdf_collect_static(object, ...)
12: spark_version(sc)
13: invoke(spark_context(sc), "version")
*** columns ***
1: (function (type, host, ...) 
{
    connection <- .rs.findActiveConnection(type, host)
    if (!is.null(connection)) 
        connection$listObjects(...)
2: connection$listObjects(...)
3: connection_list_tables(scon, includeType = TRUE)
4: sort(dbListTables(sc))
5: dbListTables(sc)
6: dbListTables(sc)
7: .local(conn, ...)
8: df_from_sql(conn, "SHOW TABLES")
9: df_from_sdf(sc, sdf)
10: sdf_collect(sdf)
11: sdf_collect_static(object, ...)
12: invoke(sdf, "columns")
*** defaultMinPartitions ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% dplyr::compute())
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    dplyr::compute()
3: eval(lhs, parent, parent)
4: eval(lhs, parent, parent)
5: sdf_len(sc, entries)
6: sdf_seq(sc, 1, length, repartition = repartition)
7: invoke(spark_context(sc), "defaultMinPartitions")
*** range ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% dplyr::compute())
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    dplyr::compute()
3: eval(lhs, parent, parent)
4: eval(lhs, parent, parent)
5: sdf_len(sc, entries)
6: sdf_seq(sc, 1, length, repartition = repartition)
7: invoke(spark_context(sc), "range", from, to, by, repartition)
*** createDataFrame ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% dplyr::compute())
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    dplyr::compute()
3: eval(lhs, parent, parent)
4: eval(lhs, parent, parent)
5: sdf_len(sc, entries)
6: sdf_seq(sc, 1, length, repartition = repartition)
7: invoke(hive_context(sc), "createDataFrame", rdd, schema)
*** registerTempTable ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% dplyr::compute())
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    dplyr::compute()
3: eval(lhs, parent, parent)
4: eval(lhs, parent, parent)
5: sdf_len(sc, entries)
6: sdf_seq(sc, 1, length, repartition = repartition)
7: sdf_register(sdf)
8: sdf_register.spark_jobj(sdf)
9: invoke(x, "registerTempTable", name)
*** sql ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% dplyr::compute())
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    dplyr::compute()
3: eval(lhs, parent, parent)
4: eval(lhs, parent, parent)
5: sdf_len(sc, entries)
6: sdf_seq(sc, 1, length, repartition = repartition)
7: sdf_register(sdf)
8: sdf_register.spark_jobj(sdf)
9: tbl(sc, name)
10: tbl.spark_connection(sc, name)
11: tbl_sql("spark", src = src, from = from, ...)
12: db_query_fields(src$con, from)
13: db_query_fields.spark_connection(src$con, from)
14: hive_context(con) %>% invoke("sql", as.character(sqlFields)) %>% 
    invoke("schema") %>% invoke("fieldNames") %>% as.character()
15: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
16: eval(quote(`_fseq`(`_lhs`)), env, env)
17: eval(quote(`_fseq`(`_lhs`)), env, env)
18: `_fseq`(`_lhs`)
19: freduce(value, `_function_list`)
20: function_list[[i]](value)
21: invoke(., "sql", as.character(sqlFields))
*** schema ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% dplyr::compute())
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    dplyr::compute()
3: eval(lhs, parent, parent)
4: eval(lhs, parent, parent)
5: sdf_len(sc, entries)
6: sdf_seq(sc, 1, length, repartition = repartition)
7: sdf_register(sdf)
8: sdf_register.spark_jobj(sdf)
9: tbl(sc, name)
10: tbl.spark_connection(sc, name)
11: tbl_sql("spark", src = src, from = from, ...)
12: db_query_fields(src$con, from)
13: db_query_fields.spark_connection(src$con, from)
14: hive_context(con) %>% invoke("sql", as.character(sqlFields)) %>% 
    invoke("schema") %>% invoke("fieldNames") %>% as.character()
15: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
16: eval(quote(`_fseq`(`_lhs`)), env, env)
17: eval(quote(`_fseq`(`_lhs`)), env, env)
18: `_fseq`(`_lhs`)
19: freduce(value, `_function_list`)
20: function_list[[i]](value)
21: invoke(., "schema")
*** fieldNames ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% dplyr::compute())
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    dplyr::compute()
3: eval(lhs, parent, parent)
4: eval(lhs, parent, parent)
5: sdf_len(sc, entries)
6: sdf_seq(sc, 1, length, repartition = repartition)
7: sdf_register(sdf)
8: sdf_register.spark_jobj(sdf)
9: tbl(sc, name)
10: tbl.spark_connection(sc, name)
11: tbl_sql("spark", src = src, from = from, ...)
12: db_query_fields(src$con, from)
13: db_query_fields.spark_connection(src$con, from)
14: hive_context(con) %>% invoke("sql", as.character(sqlFields)) %>% 
    invoke("schema") %>% invoke("fieldNames") %>% as.character()
15: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
16: eval(quote(`_fseq`(`_lhs`)), env, env)
17: eval(quote(`_fseq`(`_lhs`)), env, env)
18: `_fseq`(`_lhs`)
19: freduce(value, `_function_list`)
20: function_list[[i]](value)
21: invoke(., "fieldNames")
*** sql ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% dplyr::compute())
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    dplyr::compute()
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: dplyr::compute(.)
11: compute.tbl_sql(.)
12: db_compute(x$src$con, name, sql, temporary = temporary, unique_indexes = unique_indexes, 
    indexes = indexes, analyze = analyze, ...)
13: db_compute.DBIConnection(x$src$con, name, sql, temporary = temporary, 
    unique_indexes = unique_indexes, indexes = indexes, analyze = analyze, 
    ...)
14: db_save_query(con, sql, table, temporary = temporary)
15: db_save_query.spark_connection(con, sql, table, temporary = temporary)
16: spark_dataframe(con, sql)
17: spark_dataframe.spark_connection(con, sql)
18: invoke(hive_context(x), "sql", as.character(sql))
*** registerTempTable ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% dplyr::compute())
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    dplyr::compute()
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: dplyr::compute(.)
11: compute.tbl_sql(.)
12: db_compute(x$src$con, name, sql, temporary = temporary, unique_indexes = unique_indexes, 
    indexes = indexes, analyze = analyze, ...)
13: db_compute.DBIConnection(x$src$con, name, sql, temporary = temporary, 
    unique_indexes = unique_indexes, indexes = indexes, analyze = analyze, 
    ...)
14: db_save_query(con, sql, table, temporary = temporary)
15: db_save_query.spark_connection(con, sql, table, temporary = temporary)
16: sdf_register(df, name)
17: sdf_register.spark_jobj(df, name)
18: invoke(x, "registerTempTable", name)
*** sql ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% dplyr::compute())
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    dplyr::compute()
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: dplyr::compute(.)
11: compute.tbl_sql(.)
12: db_compute(x$src$con, name, sql, temporary = temporary, unique_indexes = unique_indexes, 
    indexes = indexes, analyze = analyze, ...)
13: db_compute.DBIConnection(x$src$con, name, sql, temporary = temporary, 
    unique_indexes = unique_indexes, indexes = indexes, analyze = analyze, 
    ...)
14: db_save_query(con, sql, table, temporary = temporary)
15: db_save_query.spark_connection(con, sql, table, temporary = temporary)
16: sdf_register(df, name)
17: sdf_register.spark_jobj(df, name)
18: tbl(sc, name)
19: tbl.spark_connection(sc, name)
20: tbl_sql("spark", src = src, from = from, ...)
21: db_query_fields(src$con, from)
22: db_query_fields.spark_connection(src$con, from)
23: hive_context(con) %>% invoke("sql", as.character(sqlFields)) %>% 
    invoke("schema") %>% invoke("fieldNames") %>% as.character()
24: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
25: eval(quote(`_fseq`(`_lhs`)), env, env)
26: eval(quote(`_fseq`(`_lhs`)), env, env)
27: `_fseq`(`_lhs`)
28: freduce(value, `_function_list`)
29: function_list[[i]](value)
30: invoke(., "sql", as.character(sqlFields))
*** schema ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% dplyr::compute())
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    dplyr::compute()
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: dplyr::compute(.)
11: compute.tbl_sql(.)
12: db_compute(x$src$con, name, sql, temporary = temporary, unique_indexes = unique_indexes, 
    indexes = indexes, analyze = analyze, ...)
13: db_compute.DBIConnection(x$src$con, name, sql, temporary = temporary, 
    unique_indexes = unique_indexes, indexes = indexes, analyze = analyze, 
    ...)
14: db_save_query(con, sql, table, temporary = temporary)
15: db_save_query.spark_connection(con, sql, table, temporary = temporary)
16: sdf_register(df, name)
17: sdf_register.spark_jobj(df, name)
18: tbl(sc, name)
19: tbl.spark_connection(sc, name)
20: tbl_sql("spark", src = src, from = from, ...)
21: db_query_fields(src$con, from)
22: db_query_fields.spark_connection(src$con, from)
23: hive_context(con) %>% invoke("sql", as.character(sqlFields)) %>% 
    invoke("schema") %>% invoke("fieldNames") %>% as.character()
24: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
25: eval(quote(`_fseq`(`_lhs`)), env, env)
26: eval(quote(`_fseq`(`_lhs`)), env, env)
27: `_fseq`(`_lhs`)
28: freduce(value, `_function_list`)
29: function_list[[i]](value)
30: invoke(., "schema")
*** fieldNames ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% dplyr::compute())
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    dplyr::compute()
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: dplyr::compute(.)
11: compute.tbl_sql(.)
12: db_compute(x$src$con, name, sql, temporary = temporary, unique_indexes = unique_indexes, 
    indexes = indexes, analyze = analyze, ...)
13: db_compute.DBIConnection(x$src$con, name, sql, temporary = temporary, 
    unique_indexes = unique_indexes, indexes = indexes, analyze = analyze, 
    ...)
14: db_save_query(con, sql, table, temporary = temporary)
15: db_save_query.spark_connection(con, sql, table, temporary = temporary)
16: sdf_register(df, name)
17: sdf_register.spark_jobj(df, name)
18: tbl(sc, name)
19: tbl.spark_connection(sc, name)
20: tbl_sql("spark", src = src, from = from, ...)
21: db_query_fields(src$con, from)
22: db_query_fields.spark_connection(src$con, from)
23: hive_context(con) %>% invoke("sql", as.character(sqlFields)) %>% 
    invoke("schema") %>% invoke("fieldNames") %>% as.character()
24: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
25: eval(quote(`_fseq`(`_lhs`)), env, env)
26: eval(quote(`_fseq`(`_lhs`)), env, env)
27: `_fseq`(`_lhs`)
28: freduce(value, `_function_list`)
29: function_list[[i]](value)
30: invoke(., "fieldNames")
*** version ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% dplyr::compute())
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    dplyr::compute()
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: dplyr::compute(.)
11: compute.tbl_sql(.)
12: db_compute(x$src$con, name, sql, temporary = temporary, unique_indexes = unique_indexes, 
    indexes = indexes, analyze = analyze, ...)
13: db_compute.DBIConnection(x$src$con, name, sql, temporary = temporary, 
    unique_indexes = unique_indexes, indexes = indexes, analyze = analyze, 
    ...)
14: db_save_query(con, sql, table, temporary = temporary)
15: db_save_query.spark_connection(con, sql, table, temporary = temporary)
16: tbl_cache(con, name)
17: spark_version(sc)
18: invoke(spark_context(sc), "version")
*** sql ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% dplyr::compute())
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    dplyr::compute()
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: dplyr::compute(.)
11: compute.tbl_sql(.)
12: db_compute(x$src$con, name, sql, temporary = temporary, unique_indexes = unique_indexes, 
    indexes = indexes, analyze = analyze, ...)
13: db_compute.DBIConnection(x$src$con, name, sql, temporary = temporary, 
    unique_indexes = unique_indexes, indexes = indexes, analyze = analyze, 
    ...)
14: db_save_query(con, sql, table, temporary = temporary)
15: db_save_query.spark_connection(con, sql, table, temporary = temporary)
16: tbl_cache(con, name)
17: tbl_cache_sql(sc, name, force)
18: invoke(hive_context(sc), "sql", sql)
*** cancelAllJobs ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% dplyr::compute())
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    dplyr::compute()
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: dplyr::compute(.)
11: compute.tbl_sql(.)
12: db_compute(x$src$con, name, sql, temporary = temporary, unique_indexes = unique_indexes, 
    indexes = indexes, analyze = analyze, ...)
13: db_compute.DBIConnection(x$src$con, name, sql, temporary = temporary, 
    unique_indexes = unique_indexes, indexes = indexes, analyze = analyze, 
    ...)
14: db_save_query(con, sql, table, temporary = temporary)
15: db_save_query.spark_connection(con, sql, table, temporary = temporary)
16: tbl_cache(con, name)
17: tbl_cache_sql(sc, name, force)
18: invoke(hive_context(sc), "sql", sql)
19: invoke.shell_jobj(hive_context(sc), "sql", sql)
20: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
21: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
22: core_invoke_method(sc, static, object, method, ...)
23: core_invoke_cancel_running(sc)
24: connection_progress_context(sc, function() {
    sc$state$cancelling_all_jobs <- TRUE
    on.exit(sc$state$cancelling_all_jobs <- FALSE)
    invoke(sc$spark_context, "cancelAllJobs")
})
25: f()
26: invoke(sc$spark_context, "cancelAllJobs")
*** sql ***
1: (function (type, host, ...) 
{
    connection <- .rs.findActiveConnection(type, host)
    if (!is.null(connection)) 
        connection$listObjects(...)
2: connection$listObjects(...)
3: connection_list_tables(scon, includeType = TRUE)
4: sort(dbListTables(sc))
5: dbListTables(sc)
6: dbListTables(sc)
7: .local(conn, ...)
8: df_from_sql(conn, "SHOW TABLES")
9: invoke(hive_context(sc), "sql", as.character(sql))
*** getSparkContext ***
1: spark_connect(master = "local")
2: initialize_connection(scon)
3: initialize_connection.spark_shell_connection(scon)
4: tryCatch({
    backend <- invoke_static(sc, "sparklyr.Shell", "getBackend")
    sc$spark_context <- invoke(backend, "getSparkContext")
    if (is.null(sc$spark_context)) {
        conf <- invoke_new(sc, "org.apache.spark.SparkConf")
5: tryCatchList(expr, classes, parentenv, handlers)
6: tryCatchOne(expr, names, parentenv, handlers[[1L]])
7: doTryCatch(return(expr), name, parentenv, handler)
8: invoke(backend, "getSparkContext")
*** setAppName ***
1: spark_connect(master = "local")
2: initialize_connection(scon)
3: initialize_connection.spark_shell_connection(scon)
4: tryCatch({
    backend <- invoke_static(sc, "sparklyr.Shell", "getBackend")
    sc$spark_context <- invoke(backend, "getSparkContext")
    if (is.null(sc$spark_context)) {
        conf <- invoke_new(sc, "org.apache.spark.SparkConf")
5: tryCatchList(expr, classes, parentenv, handlers)
6: tryCatchOne(expr, names, parentenv, handlers[[1L]])
7: doTryCatch(return(expr), name, parentenv, handler)
8: invoke(conf, "setAppName", sc$app_name)
*** setMaster ***
1: spark_connect(master = "local")
2: initialize_connection(scon)
3: initialize_connection.spark_shell_connection(scon)
4: tryCatch({
    backend <- invoke_static(sc, "sparklyr.Shell", "getBackend")
    sc$spark_context <- invoke(backend, "getSparkContext")
    if (is.null(sc$spark_context)) {
        conf <- invoke_new(sc, "org.apache.spark.SparkConf")
5: tryCatchList(expr, classes, parentenv, handlers)
6: tryCatchOne(expr, names, parentenv, handlers[[1L]])
7: doTryCatch(return(expr), name, parentenv, handler)
8: invoke(conf, "setMaster", sc$master)
*** setSparkHome ***
1: spark_connect(master = "local")
2: initialize_connection(scon)
3: initialize_connection.spark_shell_connection(scon)
4: tryCatch({
    backend <- invoke_static(sc, "sparklyr.Shell", "getBackend")
    sc$spark_context <- invoke(backend, "getSparkContext")
    if (is.null(sc$spark_context)) {
        conf <- invoke_new(sc, "org.apache.spark.SparkConf")
5: tryCatchList(expr, classes, parentenv, handlers)
6: tryCatchOne(expr, names, parentenv, handlers[[1L]])
7: doTryCatch(return(expr), name, parentenv, handler)
8: invoke(conf, "setSparkHome", sc$spark_home)
*** set ***
1: spark_connect(master = "local")
2: initialize_connection(scon)
3: initialize_connection.spark_shell_connection(scon)
4: tryCatch({
    backend <- invoke_static(sc, "sparklyr.Shell", "getBackend")
    sc$spark_context <- invoke(backend, "getSparkContext")
    if (is.null(sc$spark_context)) {
        conf <- invoke_new(sc, "org.apache.spark.SparkConf")
5: tryCatchList(expr, classes, parentenv, handlers)
6: tryCatchOne(expr, names, parentenv, handlers[[1L]])
7: doTryCatch(return(expr), name, parentenv, handler)
8: apply_config(context_config, conf, "set", "spark.")
9: lapply(names(params), function(paramName) {
    configValue <- params[[paramName]]
    if (is.logical(configValue)) {
        configValue <- if (configValue) 
            "true"
10: FUN(X[[i]], ...)
11: invoke(object, method, paste0(prefix, paramName), configValue)
*** set ***
1: spark_connect(master = "local")
2: initialize_connection(scon)
3: initialize_connection.spark_shell_connection(scon)
4: tryCatch({
    backend <- invoke_static(sc, "sparklyr.Shell", "getBackend")
    sc$spark_context <- invoke(backend, "getSparkContext")
    if (is.null(sc$spark_context)) {
        conf <- invoke_new(sc, "org.apache.spark.SparkConf")
5: tryCatchList(expr, classes, parentenv, handlers)
6: tryCatchOne(expr, names, parentenv, handlers[[1L]])
7: doTryCatch(return(expr), name, parentenv, handler)
8: apply_config(default_config, conf, "set", "spark.")
9: lapply(names(params), function(paramName) {
    configValue <- params[[paramName]]
    if (is.logical(configValue)) {
        configValue <- if (configValue) 
            "true"
10: FUN(X[[i]], ...)
11: invoke(object, method, paste0(prefix, paramName), configValue)
*** setSparkContext ***
1: spark_connect(master = "local")
2: initialize_connection(scon)
3: initialize_connection.spark_shell_connection(scon)
4: tryCatch({
    backend <- invoke_static(sc, "sparklyr.Shell", "getBackend")
    sc$spark_context <- invoke(backend, "getSparkContext")
    if (is.null(sc$spark_context)) {
        conf <- invoke_new(sc, "org.apache.spark.SparkConf")
5: tryCatchList(expr, classes, parentenv, handlers)
6: tryCatchOne(expr, names, parentenv, handlers[[1L]])
7: doTryCatch(return(expr), name, parentenv, handler)
8: invoke(backend, "setSparkContext", sc$spark_context)
*** sql ***
1: (function (type, host, ...) 
{
    connection <- .rs.findActiveConnection(type, host)
    if (!is.null(connection)) 
        connection$listObjects(...)
2: connection$listObjects(...)
3: connection_list_tables(scon, includeType = TRUE)
4: sort(dbListTables(sc))
5: dbListTables(sc)
6: dbListTables(sc)
7: .local(conn, ...)
8: df_from_sql(conn, "SHOW TABLES")
9: invoke(hive_context(sc), "sql", as.character(sql))
*** version ***
1: (function (type, host, ...) 
{
    connection <- .rs.findActiveConnection(type, host)
    if (!is.null(connection)) 
        connection$listObjects(...)
2: connection$listObjects(...)
3: connection_list_tables(scon, includeType = TRUE)
4: sort(dbListTables(sc))
5: dbListTables(sc)
6: dbListTables(sc)
7: .local(conn, ...)
8: df_from_sql(conn, "SHOW TABLES")
9: invoke(hive_context(sc), "sql", as.character(sql))
10: hive_context(sc)
11: hive_context.spark_shell_connection(sc)
12: create_hive_context(sc)
13: create_hive_context.spark_shell_connection(sc)
14: spark_version(sc)
15: invoke(spark_context(sc), "version")
*** enableHiveSupport ***
1: (function (type, host, ...) 
{
    connection <- .rs.findActiveConnection(type, host)
    if (!is.null(connection)) 
        connection$listObjects(...)
2: connection$listObjects(...)
3: connection_list_tables(scon, includeType = TRUE)
4: sort(dbListTables(sc))
5: dbListTables(sc)
6: dbListTables(sc)
7: .local(conn, ...)
8: df_from_sql(conn, "SHOW TABLES")
9: invoke(hive_context(sc), "sql", as.character(sql))
10: hive_context(sc)
11: hive_context.spark_shell_connection(sc)
12: create_hive_context(sc)
13: create_hive_context.spark_shell_connection(sc)
14: create_hive_context_v2(sc)
15: invoke(builder, "enableHiveSupport")
*** getOrCreate ***
1: (function (type, host, ...) 
{
    connection <- .rs.findActiveConnection(type, host)
    if (!is.null(connection)) 
        connection$listObjects(...)
2: connection$listObjects(...)
3: connection_list_tables(scon, includeType = TRUE)
4: sort(dbListTables(sc))
5: dbListTables(sc)
6: dbListTables(sc)
7: .local(conn, ...)
8: df_from_sql(conn, "SHOW TABLES")
9: invoke(hive_context(sc), "sql", as.character(sql))
10: hive_context(sc)
11: hive_context.spark_shell_connection(sc)
12: create_hive_context(sc)
13: create_hive_context.spark_shell_connection(sc)
14: create_hive_context_v2(sc)
15: invoke(builder, "getOrCreate")
*** conf ***
1: (function (type, host, ...) 
{
    connection <- .rs.findActiveConnection(type, host)
    if (!is.null(connection)) 
        connection$listObjects(...)
2: connection$listObjects(...)
3: connection_list_tables(scon, includeType = TRUE)
4: sort(dbListTables(sc))
5: dbListTables(sc)
6: dbListTables(sc)
7: .local(conn, ...)
8: df_from_sql(conn, "SHOW TABLES")
9: invoke(hive_context(sc), "sql", as.character(sql))
10: hive_context(sc)
11: hive_context.spark_shell_connection(sc)
12: create_hive_context(sc)
13: create_hive_context.spark_shell_connection(sc)
14: create_hive_context_v2(sc)
15: invoke(session, "conf")
*** set ***
1: (function (type, host, ...) 
{
    connection <- .rs.findActiveConnection(type, host)
    if (!is.null(connection)) 
        connection$listObjects(...)
2: connection$listObjects(...)
3: connection_list_tables(scon, includeType = TRUE)
4: sort(dbListTables(sc))
5: dbListTables(sc)
6: dbListTables(sc)
7: .local(conn, ...)
8: df_from_sql(conn, "SHOW TABLES")
9: invoke(hive_context(sc), "sql", as.character(sql))
10: hive_context(sc)
11: hive_context.spark_shell_connection(sc)
12: create_hive_context(sc)
13: create_hive_context.spark_shell_connection(sc)
14: create_hive_context_v2(sc)
15: apply_config(params, conf, "set", "spark.sql.")
16: lapply(names(params), function(paramName) {
    configValue <- params[[paramName]]
    if (is.logical(configValue)) {
        configValue <- if (configValue) 
            "true"
17: FUN(X[[i]], ...)
18: invoke(object, method, paste0(prefix, paramName), configValue)
*** version ***
1: (function (type, host, ...) 
{
    connection <- .rs.findActiveConnection(type, host)
    if (!is.null(connection)) 
        connection$listObjects(...)
2: connection$listObjects(...)
3: connection_list_tables(scon, includeType = TRUE)
4: sort(dbListTables(sc))
5: dbListTables(sc)
6: dbListTables(sc)
7: .local(conn, ...)
8: df_from_sql(conn, "SHOW TABLES")
9: df_from_sdf(sc, sdf)
10: sdf_collect(sdf)
11: sdf_is_streaming(object)
12: spark_version(sc)
13: invoke(spark_context(sc), "version")
*** isStreaming ***
1: (function (type, host, ...) 
{
    connection <- .rs.findActiveConnection(type, host)
    if (!is.null(connection)) 
        connection$listObjects(...)
2: connection$listObjects(...)
3: connection_list_tables(scon, includeType = TRUE)
4: sort(dbListTables(sc))
5: dbListTables(sc)
6: dbListTables(sc)
7: .local(conn, ...)
8: df_from_sql(conn, "SHOW TABLES")
9: df_from_sdf(sc, sdf)
10: sdf_collect(sdf)
11: sdf_is_streaming(object)
12: invoke(x, "isStreaming")
*** version ***
1: (function (type, host, ...) 
{
    connection <- .rs.findActiveConnection(type, host)
    if (!is.null(connection)) 
        connection$listObjects(...)
2: connection$listObjects(...)
3: connection_list_tables(scon, includeType = TRUE)
4: sort(dbListTables(sc))
5: dbListTables(sc)
6: dbListTables(sc)
7: .local(conn, ...)
8: df_from_sql(conn, "SHOW TABLES")
9: df_from_sdf(sc, sdf)
10: sdf_collect(sdf)
11: sdf_collect_static(object, ...)
12: spark_version(sc)
13: invoke(spark_context(sc), "version")
*** columns ***
1: (function (type, host, ...) 
{
    connection <- .rs.findActiveConnection(type, host)
    if (!is.null(connection)) 
        connection$listObjects(...)
2: connection$listObjects(...)
3: connection_list_tables(scon, includeType = TRUE)
4: sort(dbListTables(sc))
5: dbListTables(sc)
6: dbListTables(sc)
7: .local(conn, ...)
8: df_from_sql(conn, "SHOW TABLES")
9: df_from_sdf(sc, sdf)
10: sdf_collect(sdf)
11: sdf_collect_static(object, ...)
12: invoke(sdf, "columns")
*** defaultMinPartitions ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: eval(lhs, parent, parent)
4: eval(lhs, parent, parent)
5: sdf_len(sc, entries)
6: sdf_seq(sc, 1, length, repartition = repartition)
7: invoke(spark_context(sc), "defaultMinPartitions")
*** range ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: eval(lhs, parent, parent)
4: eval(lhs, parent, parent)
5: sdf_len(sc, entries)
6: sdf_seq(sc, 1, length, repartition = repartition)
7: invoke(spark_context(sc), "range", from, to, by, repartition)
*** createDataFrame ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: eval(lhs, parent, parent)
4: eval(lhs, parent, parent)
5: sdf_len(sc, entries)
6: sdf_seq(sc, 1, length, repartition = repartition)
7: invoke(hive_context(sc), "createDataFrame", rdd, schema)
*** registerTempTable ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: eval(lhs, parent, parent)
4: eval(lhs, parent, parent)
5: sdf_len(sc, entries)
6: sdf_seq(sc, 1, length, repartition = repartition)
7: sdf_register(sdf)
8: sdf_register.spark_jobj(sdf)
9: invoke(x, "registerTempTable", name)
*** sql ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: eval(lhs, parent, parent)
4: eval(lhs, parent, parent)
5: sdf_len(sc, entries)
6: sdf_seq(sc, 1, length, repartition = repartition)
7: sdf_register(sdf)
8: sdf_register.spark_jobj(sdf)
9: tbl(sc, name)
10: tbl.spark_connection(sc, name)
11: tbl_sql("spark", src = src, from = from, ...)
12: db_query_fields(src$con, from)
13: db_query_fields.spark_connection(src$con, from)
14: hive_context(con) %>% invoke("sql", as.character(sqlFields)) %>% 
    invoke("schema") %>% invoke("fieldNames") %>% as.character()
15: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
16: eval(quote(`_fseq`(`_lhs`)), env, env)
17: eval(quote(`_fseq`(`_lhs`)), env, env)
18: `_fseq`(`_lhs`)
19: freduce(value, `_function_list`)
20: function_list[[i]](value)
21: invoke(., "sql", as.character(sqlFields))
*** schema ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: eval(lhs, parent, parent)
4: eval(lhs, parent, parent)
5: sdf_len(sc, entries)
6: sdf_seq(sc, 1, length, repartition = repartition)
7: sdf_register(sdf)
8: sdf_register.spark_jobj(sdf)
9: tbl(sc, name)
10: tbl.spark_connection(sc, name)
11: tbl_sql("spark", src = src, from = from, ...)
12: db_query_fields(src$con, from)
13: db_query_fields.spark_connection(src$con, from)
14: hive_context(con) %>% invoke("sql", as.character(sqlFields)) %>% 
    invoke("schema") %>% invoke("fieldNames") %>% as.character()
15: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
16: eval(quote(`_fseq`(`_lhs`)), env, env)
17: eval(quote(`_fseq`(`_lhs`)), env, env)
18: `_fseq`(`_lhs`)
19: freduce(value, `_function_list`)
20: function_list[[i]](value)
21: invoke(., "schema")
*** fieldNames ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: eval(lhs, parent, parent)
4: eval(lhs, parent, parent)
5: sdf_len(sc, entries)
6: sdf_seq(sc, 1, length, repartition = repartition)
7: sdf_register(sdf)
8: sdf_register.spark_jobj(sdf)
9: tbl(sc, name)
10: tbl.spark_connection(sc, name)
11: tbl_sql("spark", src = src, from = from, ...)
12: db_query_fields(src$con, from)
13: db_query_fields.spark_connection(src$con, from)
14: hive_context(con) %>% invoke("sql", as.character(sqlFields)) %>% 
    invoke("schema") %>% invoke("fieldNames") %>% as.character()
15: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
16: eval(quote(`_fseq`(`_lhs`)), env, env)
17: eval(quote(`_fseq`(`_lhs`)), env, env)
18: `_fseq`(`_lhs`)
19: freduce(value, `_function_list`)
20: function_list[[i]](value)
21: invoke(., "fieldNames")
*** sql ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_sqlresult_from_dplyr(x)
13: invoke(hive_context(sc), "sql", as.character(sql))
*** write ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(df, "write")
*** option ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, "option", csvOptionName, csvOptions[[csvOptionName]])
*** option ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, "option", csvOptionName, csvOptions[[csvOptionName]])
*** option ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, "option", csvOptionName, csvOptions[[csvOptionName]])
*** option ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, "option", csvOptionName, csvOptions[[csvOptionName]])
*** option ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, "option", csvOptionName, csvOptions[[csvOptionName]])
*** option ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, "option", csvOptionName, csvOptions[[csvOptionName]])
*** option ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, "option", csvOptionName, csvOptions[[csvOptionName]])
*** csv ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
*** version ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: tryCatch({
    spark_web(sc)
}, error = function(e) {
    ""
})
31: tryCatchList(expr, classes, parentenv, handlers)
32: tryCatchOne(expr, names, parentenv, handlers[[1L]])
33: doTryCatch(return(expr), name, parentenv, handler)
34: spark_web(sc)
35: spark_version(sc)
36: invoke(spark_context(sc), "version")
*** uiWebUrl ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: tryCatch({
    spark_web(sc)
}, error = function(e) {
    ""
})
31: tryCatchList(expr, classes, parentenv, handlers)
32: tryCatchOne(expr, names, parentenv, handlers[[1L]])
33: doTryCatch(return(expr), name, parentenv, handler)
34: spark_web(sc)
35: spark_context(sc) %>% invoke("uiWebUrl") %>% invoke("isEmpty")
36: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
37: eval(quote(`_fseq`(`_lhs`)), env, env)
38: eval(quote(`_fseq`(`_lhs`)), env, env)
39: `_fseq`(`_lhs`)
40: freduce(value, `_function_list`)
41: function_list[[i]](value)
42: invoke(., "uiWebUrl")
*** isEmpty ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: tryCatch({
    spark_web(sc)
}, error = function(e) {
    ""
})
31: tryCatchList(expr, classes, parentenv, handlers)
32: tryCatchOne(expr, names, parentenv, handlers[[1L]])
33: doTryCatch(return(expr), name, parentenv, handler)
34: spark_web(sc)
35: spark_context(sc) %>% invoke("uiWebUrl") %>% invoke("isEmpty")
36: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
37: eval(quote(`_fseq`(`_lhs`)), env, env)
38: eval(quote(`_fseq`(`_lhs`)), env, env)
39: `_fseq`(`_lhs`)
40: freduce(value, `_function_list`)
41: withVisible(function_list[[k]](value))
42: function_list[[k]](value)
43: invoke(., "isEmpty")
*** uiWebUrl ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: tryCatch({
    spark_web(sc)
}, error = function(e) {
    ""
})
31: tryCatchList(expr, classes, parentenv, handlers)
32: tryCatchOne(expr, names, parentenv, handlers[[1L]])
33: doTryCatch(return(expr), name, parentenv, handler)
34: spark_web(sc)
35: spark_context(sc) %>% invoke("uiWebUrl") %>% invoke("get") %>% 
    structure(class = "spark_web_url")
36: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
37: eval(quote(`_fseq`(`_lhs`)), env, env)
38: eval(quote(`_fseq`(`_lhs`)), env, env)
39: `_fseq`(`_lhs`)
40: freduce(value, `_function_list`)
41: function_list[[i]](value)
42: invoke(., "uiWebUrl")
*** get ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: tryCatch({
    spark_web(sc)
}, error = function(e) {
    ""
})
31: tryCatchList(expr, classes, parentenv, handlers)
32: tryCatchOne(expr, names, parentenv, handlers[[1L]])
33: doTryCatch(return(expr), name, parentenv, handler)
34: spark_web(sc)
35: spark_context(sc) %>% invoke("uiWebUrl") %>% invoke("get") %>% 
    structure(class = "spark_web_url")
36: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
37: eval(quote(`_fseq`(`_lhs`)), env, env)
38: eval(quote(`_fseq`(`_lhs`)), env, env)
39: `_fseq`(`_lhs`)
40: freduce(value, `_function_list`)
41: function_list[[i]](value)
42: invoke(., "get")
*** statusTracker ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(sc$spark_context, "statusTracker")
*** getActiveJobIds ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(tracker, "getActiveJobIds")
*** getJobInfo ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(tracker, "getJobInfo", as.integer(jobId))
*** nonEmpty ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(jobInfoOption, "nonEmpty")
*** get ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(jobInfoOption, "get")
*** jobId ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(jobInfo, "jobId")
*** getJobInfo ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(tracker, "getJobInfo", as.integer(jobId))
*** nonEmpty ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(jobInfoOption, "nonEmpty")
*** get ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(jobInfoOption, "get")
*** toString ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(invoke(jobInfo, "status"), "toString")
*** status ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(invoke(jobInfo, "status"), "toString")
31: invoke(jobInfo, "status")
*** stageIds ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(jobInfo, "stageIds")
*** statusTracker ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(sc$spark_context, "statusTracker")
*** getActiveJobIds ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(tracker, "getActiveJobIds")
*** getJobInfo ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(tracker, "getJobInfo", as.integer(jobId))
*** nonEmpty ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(jobInfoOption, "nonEmpty")
*** get ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(jobInfoOption, "get")
*** jobId ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(jobInfo, "jobId")
*** getJobInfo ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(tracker, "getJobInfo", as.integer(jobId))
*** nonEmpty ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(jobInfoOption, "nonEmpty")
*** get ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(jobInfoOption, "get")
*** toString ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(invoke(jobInfo, "status"), "toString")
*** status ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(invoke(jobInfo, "status"), "toString")
31: invoke(jobInfo, "status")
*** stageIds ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(jobInfo, "stageIds")
*** getJobInfo ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(tracker, "getJobInfo", as.integer(jobId))
*** nonEmpty ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(jobInfoOption, "nonEmpty")
*** get ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(jobInfoOption, "get")
*** toString ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(invoke(jobInfo, "status"), "toString")
*** status ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(invoke(jobInfo, "status"), "toString")
31: invoke(jobInfo, "status")
*** stageIds ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(jobInfo, "stageIds")
*** statusTracker ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(sc$spark_context, "statusTracker")
*** getActiveJobIds ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(tracker, "getActiveJobIds")
*** getJobInfo ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(tracker, "getJobInfo", as.integer(jobId))
*** nonEmpty ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(jobInfoOption, "nonEmpty")
*** get ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(jobInfoOption, "get")
*** toString ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(invoke(jobInfo, "status"), "toString")
*** status ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(invoke(jobInfo, "status"), "toString")
31: invoke(jobInfo, "status")
*** stageIds ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(jobInfo, "stageIds")
*** statusTracker ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(sc$spark_context, "statusTracker")
*** getActiveJobIds ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(tracker, "getActiveJobIds")
*** getJobInfo ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(tracker, "getJobInfo", as.integer(jobId))
*** nonEmpty ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(jobInfoOption, "nonEmpty")
*** get ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(jobInfoOption, "get")
*** toString ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(invoke(jobInfo, "status"), "toString")
*** status ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(invoke(jobInfo, "status"), "toString")
31: invoke(jobInfo, "status")
*** stageIds ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(jobInfo, "stageIds")
*** statusTracker ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(sc$spark_context, "statusTracker")
*** getActiveJobIds ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(tracker, "getActiveJobIds")
*** getJobInfo ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(tracker, "getJobInfo", as.integer(jobId))
*** nonEmpty ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(jobInfoOption, "nonEmpty")
*** get ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(jobInfoOption, "get")
*** toString ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(invoke(jobInfo, "status"), "toString")
*** status ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(invoke(jobInfo, "status"), "toString")
31: invoke(jobInfo, "status")
*** stageIds ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(jobInfo, "stageIds")
*** statusTracker ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(sc$spark_context, "statusTracker")
*** getActiveJobIds ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(tracker, "getActiveJobIds")
*** getJobInfo ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(tracker, "getJobInfo", as.integer(jobId))
*** nonEmpty ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(jobInfoOption, "nonEmpty")
*** get ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(jobInfoOption, "get")
*** toString ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(invoke(jobInfo, "status"), "toString")
*** status ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(invoke(jobInfo, "status"), "toString")
31: invoke(jobInfo, "status")
*** stageIds ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(jobInfo, "stageIds")
*** statusTracker ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(sc$spark_context, "statusTracker")
*** getActiveJobIds ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(tracker, "getActiveJobIds")
*** getJobInfo ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(tracker, "getJobInfo", as.integer(jobId))
*** nonEmpty ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(jobInfoOption, "nonEmpty")
*** get ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(jobInfoOption, "get")
*** toString ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(invoke(jobInfo, "status"), "toString")
*** status ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(invoke(jobInfo, "status"), "toString")
31: invoke(jobInfo, "status")
*** stageIds ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(jobInfo, "stageIds")
*** statusTracker ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(sc$spark_context, "statusTracker")
*** getActiveJobIds ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(tracker, "getActiveJobIds")
*** getJobInfo ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(tracker, "getJobInfo", as.integer(jobId))
*** nonEmpty ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: readInt(sc)
19: read_bin(con, integer(), n = n, endian = "big")
20: read_bin.spark_connection(con, integer(), n = n, endian = "big")
21: read_bin_wait(con, what, n, endian)
22: connection_progress(sc)
23: tryCatch({
    connection_progress_base(sc, terminated)
}, error = function(e) {
})
24: tryCatchList(expr, classes, parentenv, handlers)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: doTryCatch(return(expr), name, parentenv, handler)
27: connection_progress_base(sc, terminated)
28: connection_progress_context(sc, function() {
    if (is.null(env$web_url)) {
        env$web_url <- tryCatch({
            spark_web(sc)
        }, error = function(e) {
29: f()
30: invoke(jobInfoOption, "nonEmpty")
*** getSparkContext ***
1: spark_connect(master = "local")
2: initialize_connection(scon)
3: initialize_connection.spark_shell_connection(scon)
4: tryCatch({
    backend <- invoke_static(sc, "sparklyr.Shell", "getBackend")
    sc$spark_context <- invoke(backend, "getSparkContext")
    if (is.null(sc$spark_context)) {
        conf <- invoke_new(sc, "org.apache.spark.SparkConf")
5: tryCatchList(expr, classes, parentenv, handlers)
6: tryCatchOne(expr, names, parentenv, handlers[[1L]])
7: doTryCatch(return(expr), name, parentenv, handler)
8: invoke(backend, "getSparkContext")
*** setAppName ***
1: spark_connect(master = "local")
2: initialize_connection(scon)
3: initialize_connection.spark_shell_connection(scon)
4: tryCatch({
    backend <- invoke_static(sc, "sparklyr.Shell", "getBackend")
    sc$spark_context <- invoke(backend, "getSparkContext")
    if (is.null(sc$spark_context)) {
        conf <- invoke_new(sc, "org.apache.spark.SparkConf")
5: tryCatchList(expr, classes, parentenv, handlers)
6: tryCatchOne(expr, names, parentenv, handlers[[1L]])
7: doTryCatch(return(expr), name, parentenv, handler)
8: invoke(conf, "setAppName", sc$app_name)
*** setMaster ***
1: spark_connect(master = "local")
2: initialize_connection(scon)
3: initialize_connection.spark_shell_connection(scon)
4: tryCatch({
    backend <- invoke_static(sc, "sparklyr.Shell", "getBackend")
    sc$spark_context <- invoke(backend, "getSparkContext")
    if (is.null(sc$spark_context)) {
        conf <- invoke_new(sc, "org.apache.spark.SparkConf")
5: tryCatchList(expr, classes, parentenv, handlers)
6: tryCatchOne(expr, names, parentenv, handlers[[1L]])
7: doTryCatch(return(expr), name, parentenv, handler)
8: invoke(conf, "setMaster", sc$master)
*** setSparkHome ***
1: spark_connect(master = "local")
2: initialize_connection(scon)
3: initialize_connection.spark_shell_connection(scon)
4: tryCatch({
    backend <- invoke_static(sc, "sparklyr.Shell", "getBackend")
    sc$spark_context <- invoke(backend, "getSparkContext")
    if (is.null(sc$spark_context)) {
        conf <- invoke_new(sc, "org.apache.spark.SparkConf")
5: tryCatchList(expr, classes, parentenv, handlers)
6: tryCatchOne(expr, names, parentenv, handlers[[1L]])
7: doTryCatch(return(expr), name, parentenv, handler)
8: invoke(conf, "setSparkHome", sc$spark_home)
*** set ***
1: spark_connect(master = "local")
2: initialize_connection(scon)
3: initialize_connection.spark_shell_connection(scon)
4: tryCatch({
    backend <- invoke_static(sc, "sparklyr.Shell", "getBackend")
    sc$spark_context <- invoke(backend, "getSparkContext")
    if (is.null(sc$spark_context)) {
        conf <- invoke_new(sc, "org.apache.spark.SparkConf")
5: tryCatchList(expr, classes, parentenv, handlers)
6: tryCatchOne(expr, names, parentenv, handlers[[1L]])
7: doTryCatch(return(expr), name, parentenv, handler)
8: apply_config(context_config, conf, "set", "spark.")
9: lapply(names(params), function(paramName) {
    configValue <- params[[paramName]]
    if (is.logical(configValue)) {
        configValue <- if (configValue) 
            "true"
10: FUN(X[[i]], ...)
11: invoke(object, method, paste0(prefix, paramName), configValue)
*** set ***
1: spark_connect(master = "local")
2: initialize_connection(scon)
3: initialize_connection.spark_shell_connection(scon)
4: tryCatch({
    backend <- invoke_static(sc, "sparklyr.Shell", "getBackend")
    sc$spark_context <- invoke(backend, "getSparkContext")
    if (is.null(sc$spark_context)) {
        conf <- invoke_new(sc, "org.apache.spark.SparkConf")
5: tryCatchList(expr, classes, parentenv, handlers)
6: tryCatchOne(expr, names, parentenv, handlers[[1L]])
7: doTryCatch(return(expr), name, parentenv, handler)
8: apply_config(default_config, conf, "set", "spark.")
9: lapply(names(params), function(paramName) {
    configValue <- params[[paramName]]
    if (is.logical(configValue)) {
        configValue <- if (configValue) 
            "true"
10: FUN(X[[i]], ...)
11: invoke(object, method, paste0(prefix, paramName), configValue)
*** setSparkContext ***
1: spark_connect(master = "local")
2: initialize_connection(scon)
3: initialize_connection.spark_shell_connection(scon)
4: tryCatch({
    backend <- invoke_static(sc, "sparklyr.Shell", "getBackend")
    sc$spark_context <- invoke(backend, "getSparkContext")
    if (is.null(sc$spark_context)) {
        conf <- invoke_new(sc, "org.apache.spark.SparkConf")
5: tryCatchList(expr, classes, parentenv, handlers)
6: tryCatchOne(expr, names, parentenv, handlers[[1L]])
7: doTryCatch(return(expr), name, parentenv, handler)
8: invoke(backend, "setSparkContext", sc$spark_context)
*** defaultMinPartitions ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: eval(lhs, parent, parent)
4: eval(lhs, parent, parent)
5: sdf_len(sc, entries)
6: sdf_seq(sc, 1, length, repartition = repartition)
7: invoke(spark_context(sc), "defaultMinPartitions")
*** range ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: eval(lhs, parent, parent)
4: eval(lhs, parent, parent)
5: sdf_len(sc, entries)
6: sdf_seq(sc, 1, length, repartition = repartition)
7: invoke(spark_context(sc), "range", from, to, by, repartition)
*** createDataFrame ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: eval(lhs, parent, parent)
4: eval(lhs, parent, parent)
5: sdf_len(sc, entries)
6: sdf_seq(sc, 1, length, repartition = repartition)
7: invoke(hive_context(sc), "createDataFrame", rdd, schema)
*** version ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: eval(lhs, parent, parent)
4: eval(lhs, parent, parent)
5: sdf_len(sc, entries)
6: sdf_seq(sc, 1, length, repartition = repartition)
7: invoke(hive_context(sc), "createDataFrame", rdd, schema)
8: hive_context(sc)
9: hive_context.spark_shell_connection(sc)
10: create_hive_context(sc)
11: create_hive_context.spark_shell_connection(sc)
12: spark_version(sc)
13: invoke(spark_context(sc), "version")
*** enableHiveSupport ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: eval(lhs, parent, parent)
4: eval(lhs, parent, parent)
5: sdf_len(sc, entries)
6: sdf_seq(sc, 1, length, repartition = repartition)
7: invoke(hive_context(sc), "createDataFrame", rdd, schema)
8: hive_context(sc)
9: hive_context.spark_shell_connection(sc)
10: create_hive_context(sc)
11: create_hive_context.spark_shell_connection(sc)
12: create_hive_context_v2(sc)
13: invoke(builder, "enableHiveSupport")
*** getOrCreate ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: eval(lhs, parent, parent)
4: eval(lhs, parent, parent)
5: sdf_len(sc, entries)
6: sdf_seq(sc, 1, length, repartition = repartition)
7: invoke(hive_context(sc), "createDataFrame", rdd, schema)
8: hive_context(sc)
9: hive_context.spark_shell_connection(sc)
10: create_hive_context(sc)
11: create_hive_context.spark_shell_connection(sc)
12: create_hive_context_v2(sc)
13: invoke(builder, "getOrCreate")
*** conf ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: eval(lhs, parent, parent)
4: eval(lhs, parent, parent)
5: sdf_len(sc, entries)
6: sdf_seq(sc, 1, length, repartition = repartition)
7: invoke(hive_context(sc), "createDataFrame", rdd, schema)
8: hive_context(sc)
9: hive_context.spark_shell_connection(sc)
10: create_hive_context(sc)
11: create_hive_context.spark_shell_connection(sc)
12: create_hive_context_v2(sc)
13: invoke(session, "conf")
*** set ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: eval(lhs, parent, parent)
4: eval(lhs, parent, parent)
5: sdf_len(sc, entries)
6: sdf_seq(sc, 1, length, repartition = repartition)
7: invoke(hive_context(sc), "createDataFrame", rdd, schema)
8: hive_context(sc)
9: hive_context.spark_shell_connection(sc)
10: create_hive_context(sc)
11: create_hive_context.spark_shell_connection(sc)
12: create_hive_context_v2(sc)
13: apply_config(params, conf, "set", "spark.sql.")
14: lapply(names(params), function(paramName) {
    configValue <- params[[paramName]]
    if (is.logical(configValue)) {
        configValue <- if (configValue) 
            "true"
15: FUN(X[[i]], ...)
16: invoke(object, method, paste0(prefix, paramName), configValue)
*** registerTempTable ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: eval(lhs, parent, parent)
4: eval(lhs, parent, parent)
5: sdf_len(sc, entries)
6: sdf_seq(sc, 1, length, repartition = repartition)
7: sdf_register(sdf)
8: sdf_register.spark_jobj(sdf)
9: invoke(x, "registerTempTable", name)
*** sql ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: eval(lhs, parent, parent)
4: eval(lhs, parent, parent)
5: sdf_len(sc, entries)
6: sdf_seq(sc, 1, length, repartition = repartition)
7: sdf_register(sdf)
8: sdf_register.spark_jobj(sdf)
9: tbl(sc, name)
10: tbl.spark_connection(sc, name)
11: tbl_sql("spark", src = src, from = from, ...)
12: db_query_fields(src$con, from)
13: db_query_fields.spark_connection(src$con, from)
14: hive_context(con) %>% invoke("sql", as.character(sqlFields)) %>% 
    invoke("schema") %>% invoke("fieldNames") %>% as.character()
15: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
16: eval(quote(`_fseq`(`_lhs`)), env, env)
17: eval(quote(`_fseq`(`_lhs`)), env, env)
18: `_fseq`(`_lhs`)
19: freduce(value, `_function_list`)
20: function_list[[i]](value)
21: invoke(., "sql", as.character(sqlFields))
*** schema ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: eval(lhs, parent, parent)
4: eval(lhs, parent, parent)
5: sdf_len(sc, entries)
6: sdf_seq(sc, 1, length, repartition = repartition)
7: sdf_register(sdf)
8: sdf_register.spark_jobj(sdf)
9: tbl(sc, name)
10: tbl.spark_connection(sc, name)
11: tbl_sql("spark", src = src, from = from, ...)
12: db_query_fields(src$con, from)
13: db_query_fields.spark_connection(src$con, from)
14: hive_context(con) %>% invoke("sql", as.character(sqlFields)) %>% 
    invoke("schema") %>% invoke("fieldNames") %>% as.character()
15: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
16: eval(quote(`_fseq`(`_lhs`)), env, env)
17: eval(quote(`_fseq`(`_lhs`)), env, env)
18: `_fseq`(`_lhs`)
19: freduce(value, `_function_list`)
20: function_list[[i]](value)
21: invoke(., "schema")
*** fieldNames ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: eval(lhs, parent, parent)
4: eval(lhs, parent, parent)
5: sdf_len(sc, entries)
6: sdf_seq(sc, 1, length, repartition = repartition)
7: sdf_register(sdf)
8: sdf_register.spark_jobj(sdf)
9: tbl(sc, name)
10: tbl.spark_connection(sc, name)
11: tbl_sql("spark", src = src, from = from, ...)
12: db_query_fields(src$con, from)
13: db_query_fields.spark_connection(src$con, from)
14: hive_context(con) %>% invoke("sql", as.character(sqlFields)) %>% 
    invoke("schema") %>% invoke("fieldNames") %>% as.character()
15: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
16: eval(quote(`_fseq`(`_lhs`)), env, env)
17: eval(quote(`_fseq`(`_lhs`)), env, env)
18: `_fseq`(`_lhs`)
19: freduce(value, `_function_list`)
20: function_list[[i]](value)
21: invoke(., "fieldNames")
*** sql ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_sqlresult_from_dplyr(x)
13: invoke(hive_context(sc), "sql", as.character(sql))
*** write ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(df, "write")
*** option ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, "option", csvOptionName, csvOptions[[csvOptionName]])
*** option ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, "option", csvOptionName, csvOptions[[csvOptionName]])
*** option ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, "option", csvOptionName, csvOptions[[csvOptionName]])
*** option ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, "option", csvOptionName, csvOptions[[csvOptionName]])
*** option ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, "option", csvOptionName, csvOptions[[csvOptionName]])
*** option ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, "option", csvOptionName, csvOptions[[csvOptionName]])
*** option ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, "option", csvOptionName, csvOptions[[csvOptionName]])
*** csv ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
*** cancelAllJobs ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: core_invoke_cancel_running(sc)
19: connection_progress_context(sc, function() {
    sc$state$cancelling_all_jobs <- TRUE
    on.exit(sc$state$cancelling_all_jobs <- FALSE)
    invoke(sc$spark_context, "cancelAllJobs")
})
20: f()
21: invoke(sc$spark_context, "cancelAllJobs")
*** defaultMinPartitions ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: eval(lhs, parent, parent)
4: eval(lhs, parent, parent)
5: sdf_len(sc, entries)
6: sdf_seq(sc, 1, length, repartition = repartition)
7: invoke(spark_context(sc), "defaultMinPartitions")
*** range ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: eval(lhs, parent, parent)
4: eval(lhs, parent, parent)
5: sdf_len(sc, entries)
6: sdf_seq(sc, 1, length, repartition = repartition)
7: invoke(spark_context(sc), "range", from, to, by, repartition)
*** createDataFrame ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: eval(lhs, parent, parent)
4: eval(lhs, parent, parent)
5: sdf_len(sc, entries)
6: sdf_seq(sc, 1, length, repartition = repartition)
7: invoke(hive_context(sc), "createDataFrame", rdd, schema)
*** registerTempTable ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: eval(lhs, parent, parent)
4: eval(lhs, parent, parent)
5: sdf_len(sc, entries)
6: sdf_seq(sc, 1, length, repartition = repartition)
7: sdf_register(sdf)
8: sdf_register.spark_jobj(sdf)
9: invoke(x, "registerTempTable", name)
*** sql ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: eval(lhs, parent, parent)
4: eval(lhs, parent, parent)
5: sdf_len(sc, entries)
6: sdf_seq(sc, 1, length, repartition = repartition)
7: sdf_register(sdf)
8: sdf_register.spark_jobj(sdf)
9: tbl(sc, name)
10: tbl.spark_connection(sc, name)
11: tbl_sql("spark", src = src, from = from, ...)
12: db_query_fields(src$con, from)
13: db_query_fields.spark_connection(src$con, from)
14: hive_context(con) %>% invoke("sql", as.character(sqlFields)) %>% 
    invoke("schema") %>% invoke("fieldNames") %>% as.character()
15: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
16: eval(quote(`_fseq`(`_lhs`)), env, env)
17: eval(quote(`_fseq`(`_lhs`)), env, env)
18: `_fseq`(`_lhs`)
19: freduce(value, `_function_list`)
20: function_list[[i]](value)
21: invoke(., "sql", as.character(sqlFields))
*** schema ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: eval(lhs, parent, parent)
4: eval(lhs, parent, parent)
5: sdf_len(sc, entries)
6: sdf_seq(sc, 1, length, repartition = repartition)
7: sdf_register(sdf)
8: sdf_register.spark_jobj(sdf)
9: tbl(sc, name)
10: tbl.spark_connection(sc, name)
11: tbl_sql("spark", src = src, from = from, ...)
12: db_query_fields(src$con, from)
13: db_query_fields.spark_connection(src$con, from)
14: hive_context(con) %>% invoke("sql", as.character(sqlFields)) %>% 
    invoke("schema") %>% invoke("fieldNames") %>% as.character()
15: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
16: eval(quote(`_fseq`(`_lhs`)), env, env)
17: eval(quote(`_fseq`(`_lhs`)), env, env)
18: `_fseq`(`_lhs`)
19: freduce(value, `_function_list`)
20: function_list[[i]](value)
21: invoke(., "schema")
*** fieldNames ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: eval(lhs, parent, parent)
4: eval(lhs, parent, parent)
5: sdf_len(sc, entries)
6: sdf_seq(sc, 1, length, repartition = repartition)
7: sdf_register(sdf)
8: sdf_register.spark_jobj(sdf)
9: tbl(sc, name)
10: tbl.spark_connection(sc, name)
11: tbl_sql("spark", src = src, from = from, ...)
12: db_query_fields(src$con, from)
13: db_query_fields.spark_connection(src$con, from)
14: hive_context(con) %>% invoke("sql", as.character(sqlFields)) %>% 
    invoke("schema") %>% invoke("fieldNames") %>% as.character()
15: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
16: eval(quote(`_fseq`(`_lhs`)), env, env)
17: eval(quote(`_fseq`(`_lhs`)), env, env)
18: `_fseq`(`_lhs`)
19: freduce(value, `_function_list`)
20: function_list[[i]](value)
21: invoke(., "fieldNames")
*** sql ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_sqlresult_from_dplyr(x)
13: invoke(hive_context(sc), "sql", as.character(sql))
*** write ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(df, "write")
*** option ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, "option", csvOptionName, csvOptions[[csvOptionName]])
*** option ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, "option", csvOptionName, csvOptions[[csvOptionName]])
*** option ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, "option", csvOptionName, csvOptions[[csvOptionName]])
*** option ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, "option", csvOptionName, csvOptions[[csvOptionName]])
*** option ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, "option", csvOptionName, csvOptions[[csvOptionName]])
*** option ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, "option", csvOptionName, csvOptions[[csvOptionName]])
*** option ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, "option", csvOptionName, csvOptions[[csvOptionName]])
*** csv ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
*** cancelAllJobs ***
1: system.time(sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries) %>% dplyr::mutate(x = rand()) %>% dplyr::arrange(x) %>% 
    spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: core_invoke_cancel_running(sc)
19: connection_progress_context(sc, function() {
    sc$state$cancelling_all_jobs <- TRUE
    on.exit(sc$state$cancelling_all_jobs <- FALSE)
    invoke(sc$spark_context, "cancelAllJobs")
})
20: f()
21: invoke(sc$spark_context, "cancelAllJobs")
*** getSparkContext ***
1: spark_connect(master = "local")
2: initialize_connection(scon)
3: initialize_connection.spark_shell_connection(scon)
4: tryCatch({
    backend <- invoke_static(sc, "sparklyr.Shell", "getBackend")
    sc$spark_context <- invoke(backend, "getSparkContext")
    if (is.null(sc$spark_context)) {
        conf <- invoke_new(sc, "org.apache.spark.SparkConf")
5: tryCatchList(expr, classes, parentenv, handlers)
6: tryCatchOne(expr, names, parentenv, handlers[[1L]])
7: doTryCatch(return(expr), name, parentenv, handler)
8: invoke(backend, "getSparkContext")
*** setAppName ***
1: spark_connect(master = "local")
2: initialize_connection(scon)
3: initialize_connection.spark_shell_connection(scon)
4: tryCatch({
    backend <- invoke_static(sc, "sparklyr.Shell", "getBackend")
    sc$spark_context <- invoke(backend, "getSparkContext")
    if (is.null(sc$spark_context)) {
        conf <- invoke_new(sc, "org.apache.spark.SparkConf")
5: tryCatchList(expr, classes, parentenv, handlers)
6: tryCatchOne(expr, names, parentenv, handlers[[1L]])
7: doTryCatch(return(expr), name, parentenv, handler)
8: invoke(conf, "setAppName", sc$app_name)
*** setMaster ***
1: spark_connect(master = "local")
2: initialize_connection(scon)
3: initialize_connection.spark_shell_connection(scon)
4: tryCatch({
    backend <- invoke_static(sc, "sparklyr.Shell", "getBackend")
    sc$spark_context <- invoke(backend, "getSparkContext")
    if (is.null(sc$spark_context)) {
        conf <- invoke_new(sc, "org.apache.spark.SparkConf")
5: tryCatchList(expr, classes, parentenv, handlers)
6: tryCatchOne(expr, names, parentenv, handlers[[1L]])
7: doTryCatch(return(expr), name, parentenv, handler)
8: invoke(conf, "setMaster", sc$master)
*** setSparkHome ***
1: spark_connect(master = "local")
2: initialize_connection(scon)
3: initialize_connection.spark_shell_connection(scon)
4: tryCatch({
    backend <- invoke_static(sc, "sparklyr.Shell", "getBackend")
    sc$spark_context <- invoke(backend, "getSparkContext")
    if (is.null(sc$spark_context)) {
        conf <- invoke_new(sc, "org.apache.spark.SparkConf")
5: tryCatchList(expr, classes, parentenv, handlers)
6: tryCatchOne(expr, names, parentenv, handlers[[1L]])
7: doTryCatch(return(expr), name, parentenv, handler)
8: invoke(conf, "setSparkHome", sc$spark_home)
*** set ***
1: spark_connect(master = "local")
2: initialize_connection(scon)
3: initialize_connection.spark_shell_connection(scon)
4: tryCatch({
    backend <- invoke_static(sc, "sparklyr.Shell", "getBackend")
    sc$spark_context <- invoke(backend, "getSparkContext")
    if (is.null(sc$spark_context)) {
        conf <- invoke_new(sc, "org.apache.spark.SparkConf")
5: tryCatchList(expr, classes, parentenv, handlers)
6: tryCatchOne(expr, names, parentenv, handlers[[1L]])
7: doTryCatch(return(expr), name, parentenv, handler)
8: apply_config(context_config, conf, "set", "spark.")
9: lapply(names(params), function(paramName) {
    configValue <- params[[paramName]]
    if (is.logical(configValue)) {
        configValue <- if (configValue) 
            "true"
10: FUN(X[[i]], ...)
11: invoke(object, method, paste0(prefix, paramName), configValue)
*** set ***
1: spark_connect(master = "local")
2: initialize_connection(scon)
3: initialize_connection.spark_shell_connection(scon)
4: tryCatch({
    backend <- invoke_static(sc, "sparklyr.Shell", "getBackend")
    sc$spark_context <- invoke(backend, "getSparkContext")
    if (is.null(sc$spark_context)) {
        conf <- invoke_new(sc, "org.apache.spark.SparkConf")
5: tryCatchList(expr, classes, parentenv, handlers)
6: tryCatchOne(expr, names, parentenv, handlers[[1L]])
7: doTryCatch(return(expr), name, parentenv, handler)
8: apply_config(default_config, conf, "set", "spark.")
9: lapply(names(params), function(paramName) {
    configValue <- params[[paramName]]
    if (is.logical(configValue)) {
        configValue <- if (configValue) 
            "true"
10: FUN(X[[i]], ...)
11: invoke(object, method, paste0(prefix, paramName), configValue)
*** setSparkContext ***
1: spark_connect(master = "local")
2: initialize_connection(scon)
3: initialize_connection.spark_shell_connection(scon)
4: tryCatch({
    backend <- invoke_static(sc, "sparklyr.Shell", "getBackend")
    sc$spark_context <- invoke(backend, "getSparkContext")
    if (is.null(sc$spark_context)) {
        conf <- invoke_new(sc, "org.apache.spark.SparkConf")
5: tryCatchList(expr, classes, parentenv, handlers)
6: tryCatchOne(expr, names, parentenv, handlers[[1L]])
7: doTryCatch(return(expr), name, parentenv, handler)
8: invoke(backend, "setSparkContext", sc$spark_context)
*** range ***
1: system.time(sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv")
3: eval(lhs, parent, parent)
4: eval(lhs, parent, parent)
5: sdf_len(sc, entries, repartition = 10000)
6: sdf_seq(sc, 1, length, repartition = repartition)
7: invoke(spark_context(sc), "range", from, to, by, repartition)
*** createDataFrame ***
1: system.time(sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv")
3: eval(lhs, parent, parent)
4: eval(lhs, parent, parent)
5: sdf_len(sc, entries, repartition = 10000)
6: sdf_seq(sc, 1, length, repartition = repartition)
7: invoke(hive_context(sc), "createDataFrame", rdd, schema)
*** version ***
1: system.time(sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv")
3: eval(lhs, parent, parent)
4: eval(lhs, parent, parent)
5: sdf_len(sc, entries, repartition = 10000)
6: sdf_seq(sc, 1, length, repartition = repartition)
7: invoke(hive_context(sc), "createDataFrame", rdd, schema)
8: hive_context(sc)
9: hive_context.spark_shell_connection(sc)
10: create_hive_context(sc)
11: create_hive_context.spark_shell_connection(sc)
12: spark_version(sc)
13: invoke(spark_context(sc), "version")
*** enableHiveSupport ***
1: system.time(sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv")
3: eval(lhs, parent, parent)
4: eval(lhs, parent, parent)
5: sdf_len(sc, entries, repartition = 10000)
6: sdf_seq(sc, 1, length, repartition = repartition)
7: invoke(hive_context(sc), "createDataFrame", rdd, schema)
8: hive_context(sc)
9: hive_context.spark_shell_connection(sc)
10: create_hive_context(sc)
11: create_hive_context.spark_shell_connection(sc)
12: create_hive_context_v2(sc)
13: invoke(builder, "enableHiveSupport")
*** getOrCreate ***
1: system.time(sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv")
3: eval(lhs, parent, parent)
4: eval(lhs, parent, parent)
5: sdf_len(sc, entries, repartition = 10000)
6: sdf_seq(sc, 1, length, repartition = repartition)
7: invoke(hive_context(sc), "createDataFrame", rdd, schema)
8: hive_context(sc)
9: hive_context.spark_shell_connection(sc)
10: create_hive_context(sc)
11: create_hive_context.spark_shell_connection(sc)
12: create_hive_context_v2(sc)
13: invoke(builder, "getOrCreate")
*** conf ***
1: system.time(sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv")
3: eval(lhs, parent, parent)
4: eval(lhs, parent, parent)
5: sdf_len(sc, entries, repartition = 10000)
6: sdf_seq(sc, 1, length, repartition = repartition)
7: invoke(hive_context(sc), "createDataFrame", rdd, schema)
8: hive_context(sc)
9: hive_context.spark_shell_connection(sc)
10: create_hive_context(sc)
11: create_hive_context.spark_shell_connection(sc)
12: create_hive_context_v2(sc)
13: invoke(session, "conf")
*** set ***
1: system.time(sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv")
3: eval(lhs, parent, parent)
4: eval(lhs, parent, parent)
5: sdf_len(sc, entries, repartition = 10000)
6: sdf_seq(sc, 1, length, repartition = repartition)
7: invoke(hive_context(sc), "createDataFrame", rdd, schema)
8: hive_context(sc)
9: hive_context.spark_shell_connection(sc)
10: create_hive_context(sc)
11: create_hive_context.spark_shell_connection(sc)
12: create_hive_context_v2(sc)
13: apply_config(params, conf, "set", "spark.sql.")
14: lapply(names(params), function(paramName) {
    configValue <- params[[paramName]]
    if (is.logical(configValue)) {
        configValue <- if (configValue) 
            "true"
15: FUN(X[[i]], ...)
16: invoke(object, method, paste0(prefix, paramName), configValue)
*** registerTempTable ***
1: system.time(sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv")
3: eval(lhs, parent, parent)
4: eval(lhs, parent, parent)
5: sdf_len(sc, entries, repartition = 10000)
6: sdf_seq(sc, 1, length, repartition = repartition)
7: sdf_register(sdf)
8: sdf_register.spark_jobj(sdf)
9: invoke(x, "registerTempTable", name)
*** sql ***
1: system.time(sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv")
3: eval(lhs, parent, parent)
4: eval(lhs, parent, parent)
5: sdf_len(sc, entries, repartition = 10000)
6: sdf_seq(sc, 1, length, repartition = repartition)
7: sdf_register(sdf)
8: sdf_register.spark_jobj(sdf)
9: tbl(sc, name)
10: tbl.spark_connection(sc, name)
11: tbl_sql("spark", src = src, from = from, ...)
12: db_query_fields(src$con, from)
13: db_query_fields.spark_connection(src$con, from)
14: hive_context(con) %>% invoke("sql", as.character(sqlFields)) %>% 
    invoke("schema") %>% invoke("fieldNames") %>% as.character()
15: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
16: eval(quote(`_fseq`(`_lhs`)), env, env)
17: eval(quote(`_fseq`(`_lhs`)), env, env)
18: `_fseq`(`_lhs`)
19: freduce(value, `_function_list`)
20: function_list[[i]](value)
21: invoke(., "sql", as.character(sqlFields))
*** schema ***
1: system.time(sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv")
3: eval(lhs, parent, parent)
4: eval(lhs, parent, parent)
5: sdf_len(sc, entries, repartition = 10000)
6: sdf_seq(sc, 1, length, repartition = repartition)
7: sdf_register(sdf)
8: sdf_register.spark_jobj(sdf)
9: tbl(sc, name)
10: tbl.spark_connection(sc, name)
11: tbl_sql("spark", src = src, from = from, ...)
12: db_query_fields(src$con, from)
13: db_query_fields.spark_connection(src$con, from)
14: hive_context(con) %>% invoke("sql", as.character(sqlFields)) %>% 
    invoke("schema") %>% invoke("fieldNames") %>% as.character()
15: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
16: eval(quote(`_fseq`(`_lhs`)), env, env)
17: eval(quote(`_fseq`(`_lhs`)), env, env)
18: `_fseq`(`_lhs`)
19: freduce(value, `_function_list`)
20: function_list[[i]](value)
21: invoke(., "schema")
*** fieldNames ***
1: system.time(sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv")
3: eval(lhs, parent, parent)
4: eval(lhs, parent, parent)
5: sdf_len(sc, entries, repartition = 10000)
6: sdf_seq(sc, 1, length, repartition = repartition)
7: sdf_register(sdf)
8: sdf_register.spark_jobj(sdf)
9: tbl(sc, name)
10: tbl.spark_connection(sc, name)
11: tbl_sql("spark", src = src, from = from, ...)
12: db_query_fields(src$con, from)
13: db_query_fields.spark_connection(src$con, from)
14: hive_context(con) %>% invoke("sql", as.character(sqlFields)) %>% 
    invoke("schema") %>% invoke("fieldNames") %>% as.character()
15: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
16: eval(quote(`_fseq`(`_lhs`)), env, env)
17: eval(quote(`_fseq`(`_lhs`)), env, env)
18: `_fseq`(`_lhs`)
19: freduce(value, `_function_list`)
20: function_list[[i]](value)
21: invoke(., "fieldNames")
*** sql ***
1: system.time(sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_sqlresult_from_dplyr(x)
13: invoke(hive_context(sc), "sql", as.character(sql))
*** write ***
1: system.time(sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(df, "write")
*** option ***
1: system.time(sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, "option", csvOptionName, csvOptions[[csvOptionName]])
*** option ***
1: system.time(sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, "option", csvOptionName, csvOptions[[csvOptionName]])
*** option ***
1: system.time(sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, "option", csvOptionName, csvOptions[[csvOptionName]])
*** option ***
1: system.time(sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, "option", csvOptionName, csvOptions[[csvOptionName]])
*** option ***
1: system.time(sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, "option", csvOptionName, csvOptions[[csvOptionName]])
*** option ***
1: system.time(sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, "option", csvOptionName, csvOptions[[csvOptionName]])
*** option ***
1: system.time(sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, "option", csvOptionName, csvOptions[[csvOptionName]])
*** csv ***
1: system.time(sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
*** cancelAllJobs ***
1: system.time(sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
14: invoke.shell_jobj(options, spark_csv_save_name(sc), path)
15: invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)
16: invoke_method.spark_shell_connection(spark_connection(jobj), 
    FALSE, jobj, method, ...)
17: core_invoke_method(sc, static, object, method, ...)
18: core_invoke_cancel_running(sc)
19: connection_progress_context(sc, function() {
    sc$state$cancelling_all_jobs <- TRUE
    on.exit(sc$state$cancelling_all_jobs <- FALSE)
    invoke(sc$spark_context, "cancelAllJobs")
})
20: f()
21: invoke(sc$spark_context, "cancelAllJobs")
*** range ***
1: system.time(sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv")
3: eval(lhs, parent, parent)
4: eval(lhs, parent, parent)
5: sdf_len(sc, entries, repartition = 10000)
6: sdf_seq(sc, 1, length, repartition = repartition)
7: invoke(spark_context(sc), "range", from, to, by, repartition)
*** createDataFrame ***
1: system.time(sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv")
3: eval(lhs, parent, parent)
4: eval(lhs, parent, parent)
5: sdf_len(sc, entries, repartition = 10000)
6: sdf_seq(sc, 1, length, repartition = repartition)
7: invoke(hive_context(sc), "createDataFrame", rdd, schema)
*** registerTempTable ***
1: system.time(sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv")
3: eval(lhs, parent, parent)
4: eval(lhs, parent, parent)
5: sdf_len(sc, entries, repartition = 10000)
6: sdf_seq(sc, 1, length, repartition = repartition)
7: sdf_register(sdf)
8: sdf_register.spark_jobj(sdf)
9: invoke(x, "registerTempTable", name)
*** sql ***
1: system.time(sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv")
3: eval(lhs, parent, parent)
4: eval(lhs, parent, parent)
5: sdf_len(sc, entries, repartition = 10000)
6: sdf_seq(sc, 1, length, repartition = repartition)
7: sdf_register(sdf)
8: sdf_register.spark_jobj(sdf)
9: tbl(sc, name)
10: tbl.spark_connection(sc, name)
11: tbl_sql("spark", src = src, from = from, ...)
12: db_query_fields(src$con, from)
13: db_query_fields.spark_connection(src$con, from)
14: hive_context(con) %>% invoke("sql", as.character(sqlFields)) %>% 
    invoke("schema") %>% invoke("fieldNames") %>% as.character()
15: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
16: eval(quote(`_fseq`(`_lhs`)), env, env)
17: eval(quote(`_fseq`(`_lhs`)), env, env)
18: `_fseq`(`_lhs`)
19: freduce(value, `_function_list`)
20: function_list[[i]](value)
21: invoke(., "sql", as.character(sqlFields))
*** schema ***
1: system.time(sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv")
3: eval(lhs, parent, parent)
4: eval(lhs, parent, parent)
5: sdf_len(sc, entries, repartition = 10000)
6: sdf_seq(sc, 1, length, repartition = repartition)
7: sdf_register(sdf)
8: sdf_register.spark_jobj(sdf)
9: tbl(sc, name)
10: tbl.spark_connection(sc, name)
11: tbl_sql("spark", src = src, from = from, ...)
12: db_query_fields(src$con, from)
13: db_query_fields.spark_connection(src$con, from)
14: hive_context(con) %>% invoke("sql", as.character(sqlFields)) %>% 
    invoke("schema") %>% invoke("fieldNames") %>% as.character()
15: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
16: eval(quote(`_fseq`(`_lhs`)), env, env)
17: eval(quote(`_fseq`(`_lhs`)), env, env)
18: `_fseq`(`_lhs`)
19: freduce(value, `_function_list`)
20: function_list[[i]](value)
21: invoke(., "schema")
*** fieldNames ***
1: system.time(sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv")
3: eval(lhs, parent, parent)
4: eval(lhs, parent, parent)
5: sdf_len(sc, entries, repartition = 10000)
6: sdf_seq(sc, 1, length, repartition = repartition)
7: sdf_register(sdf)
8: sdf_register.spark_jobj(sdf)
9: tbl(sc, name)
10: tbl.spark_connection(sc, name)
11: tbl_sql("spark", src = src, from = from, ...)
12: db_query_fields(src$con, from)
13: db_query_fields.spark_connection(src$con, from)
14: hive_context(con) %>% invoke("sql", as.character(sqlFields)) %>% 
    invoke("schema") %>% invoke("fieldNames") %>% as.character()
15: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
16: eval(quote(`_fseq`(`_lhs`)), env, env)
17: eval(quote(`_fseq`(`_lhs`)), env, env)
18: `_fseq`(`_lhs`)
19: freduce(value, `_function_list`)
20: function_list[[i]](value)
21: invoke(., "fieldNames")
*** sql ***
1: system.time(sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_sqlresult_from_dplyr(x)
13: invoke(hive_context(sc), "sql", as.character(sql))
*** write ***
1: system.time(sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(df, "write")
*** option ***
1: system.time(sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, "option", csvOptionName, csvOptions[[csvOptionName]])
*** option ***
1: system.time(sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, "option", csvOptionName, csvOptions[[csvOptionName]])
*** option ***
1: system.time(sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, "option", csvOptionName, csvOptions[[csvOptionName]])
*** option ***
1: system.time(sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, "option", csvOptionName, csvOptions[[csvOptionName]])
*** option ***
1: system.time(sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, "option", csvOptionName, csvOptions[[csvOptionName]])
*** option ***
1: system.time(sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, "option", csvOptionName, csvOptions[[csvOptionName]])
*** option ***
1: system.time(sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, "option", csvOptionName, csvOptions[[csvOptionName]])
*** csv ***
1: system.time(sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv"))
2: sdf_len(sc, entries, repartition = 10000) %>% dplyr::mutate(x = rand()) %>% 
    dplyr::arrange(x) %>% spark_write_csv("100m.csv")
3: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
4: eval(quote(`_fseq`(`_lhs`)), env, env)
5: eval(quote(`_fseq`(`_lhs`)), env, env)
6: `_fseq`(`_lhs`)
7: freduce(value, `_function_list`)
8: withVisible(function_list[[k]](value))
9: function_list[[k]](value)
10: spark_write_csv(., "100m.csv")
11: spark_write_csv.tbl_spark(., "100m.csv")
12: spark_csv_write(sqlResult, spark_normalize_path(path), options, 
    mode, partition_by)
13: invoke(options, spark_csv_save_name(sc), path)
