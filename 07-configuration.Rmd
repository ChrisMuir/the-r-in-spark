# Configuration {#configuration}

**This chatper has not been written.**

The previous chapter, [Connections], explained how to connect to Spark clusters; however, it is often the case that connection is not sufficient to run proper analysis at scale since the default settings (say number of executors, avaialble memory, etc) that a system administrator configured need to be modified to your particular needs. This chapter will explain how to configure, what can be configured and the important use cases you want to consider while configuring your Spark environment.

## Overview

```{r eval=FALSE}
config <- spark_config()
sc <- spark_connect(master = "local")

spark_context_config(sc)

spark_session_config(sc)

# Previous versions
spark_hive_config(sc)
```

## Tools

## Resources

https://spark.apache.org/docs/latest/configuration.html
