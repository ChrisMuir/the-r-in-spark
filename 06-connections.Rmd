# Connections {#connections}

The previous chapter, Clusters, presented the major cluster computing paradigms, cluster managers and cluster providers; this section explains the components of a Spark cluster regardless of where the cluster is located or managed and the how to perform connections to any cluster running Apache Spark.

## Overview

Before explaining how to connect to Spark clusters, it is worth discussing the components of a Spark cluster and how they interact, this is often known as the cluster architecture of Apache Spark.

In a Spark cluster, there are two main components, meaning, there are two types of compute instances that are relevant to Spark: The **driver node** and the **worker nodes**.

If you already have an Spark cluster in their organization, you should ask your cluster administrator to provide connection information for this cluster and read carefully their usage policies and constraints. A cluster is usually shared among many users so you want to be respectful of others time and resources while using a shared cluster environment. Your system administrator will describe if it's an **on-premise** vs **cloud** cluster, the **distribution**, the cluster **manager** being used, supported **connections** and supported **tools**.



## Local

## Yarn

### Client

### Server

## Mesos

## Livy

```{r apache-livy, fig.width = 4, fig.align = 'center', echo=FALSE, fig.cap='Apache Livy.'}
knitr::include_graphics("images/05-clusters-apache-livy.png")
```
