# Streaming {#streaming}

## Overview

One can understand a stream as an unbound data frame, meaning, a data frame with finite columns but infinite rows. Streams are most relevant when processing real time data; for example, when analyzing a Twitter feed or stock prices. Both examples have well defined columns, like 'tweet' or 'price', but there are always new rows of data to be analyzed.

Spark provided initial support for streams with Spark's DStreams; however, a more versatile and efficient replacement is available through [Spark structured streams](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html). Structured streams provide scalable and fault-torerant data processing over streams of data. Meaning that, one can use many machines to process multiple streaming sources, perform joins with other streams or static sources, and recover from failures with at-least-once guarantees (where each message is certain to be delivered, but may do so multiple times).

In order to use structured streams in `sparklyr`, one needs to define the **sources**, **transformations** and a **destination**:

* The **sources** are defined using any of the `stream_read_*()` functions to read streams of data from various data sources.
* The **transformations** can be specified using `dplyr`, `SQL`, scoring pipelines or R code through `spark_apply()`.
* The **destination** is defined with the `stream_write_*()` functions, it often also referenced as a sink.

Since the transformation step is optional, the simplest stream we can define is to continuously process files, which would effectively copy text files between source and destination. We can define this copy-stream in `sparklyr` as follows:

```{r echo=FALSE}
unlink(c("source/", "destination/"), recursive = TRUE)
dir.create("source")
```
```{r}
library(sparklyr)
sc <- spark_connect(master = "local")
stream <- stream_read_text(sc, "source/") %>% stream_write_text("destination/")
```

Once executed, the stream will monitor the `source` path and process data into the `destination/` path as it arrives. We can use `view_stream()` to track the **rows per second (rps)** being processed in the source, destination and their latest values over time:
```{r eval=FALSE}
stream_view(stream)
```
```{r echo=FALSE, eval=FALSE}
stream_artificial_stats <- function(stream, path) {
  unlink(c("source/"), recursive = TRUE)
  dir.create("source")
  stats <- stream_stats(stream)
  
  dist <- floor(10 + 1e5 * (dbinom(1:50, 50, 0.7) + dbinom(1:50, 50, 0.3)))
  
  for (i in seq_along(dist)) {
    writeLines(paste("Row", 1:dist[i]), paste0("source/hello_", i, ".txt"))
   
    Sys.sleep(1)
    stats <- stream_stats(stream, stats)
  }
  
  stats$sources <- gsub("/Users/.*the-r-in-spark/", "", stats$sources)
  saveRDS(stats, path)
}
stream_artificial_stats(stream, "data/10-streaming-overview.rds")
```
```{r echo=FALSE, fig.align = 'center', out.width='100%', out.height='280pt', fig.cap='Viewing a Spark Stream with sparklyr'}
library(sparklyr)
readRDS("data/10-streaming-overview.rds") %>% stream_render(stats = .)
```

Notice that the rows-per-second in the destination stream are higher than the rows-per-second in the source stream; this is expected and desireable since Spark measures incoming rates from the source, but actual row processing times in the destination stream.

Use `stream_stop()` to properly stop processing data from this stream:

```{r eval=FALSE}
stream_stop(stream)
```

Notice that, in order to reproduce the above example, one needs to feed streaming data into the `source/` path. This was accomplished by running, before `stream_view(stream)`, the following script to produce a file every second containing lines of text that follow two overlapping binomial distributions. In practice, you would connect to existing sources without having to generate data artificially.

```{r eval=FALSE}
unlink(c("source/", "destination/"), recursive = TRUE)
dir.create("source")
callr::r_bg(function() {
  dist <- floor(10 + 1e5 * (dbinom(1:50, 50, 0.7) + dbinom(1:50, 50, 0.3)))
  for (i in seq_len(10)) {
    writeLines(paste("Row", 1:dist[i]), paste0("source/hello_", i, ".txt"))
    Sys.sleep(1)
  }
})
```

For the subsequent examples, a stream with one hundred rows of text will be used:

```{r}
writeLines(paste("Row", 1:100), "source/rows.txt")
```

## Transformations

Streams can be transformed using `dplyr`, SQL, pipelines or R code. We can use as many transformations as needed in the same way that Spark data frames can be transformed with `sparklyr`. The transformation source can be streams or data frames but the output is always a stream. If needed, one can always take a snapshot from the destination stream and save the output as a data frame, which is what `sparklyr` will do for you if a destination stream is not specified. Conceptually, this looks as follows:

```{r echo=FALSE, message=FALSE, fig.align = 'center', out.width='100%', out.height='280pt', fig.cap='Streams Transformation Diagram'}
r2d3::r2d3(
  c(),
  file.path("images", "10-streaming-transformations.js"),
  dependencies = "images/06-connections-diagram.js",
  css = "images/06-connections-diagram.css"
)
```

### dplyr {#streams-dplyr}

Using `dplyr`, we can process each row of the stream; for example, we can filter the stream to only the rows containing a number one:

```{r}
library(dplyr, warn.conflicts = FALSE)

stream_read_text(sc, "source/") %>%
  filter(text %like% "%1%")
```

Since the destination was not specified, `sparklyr` creates a temporary memory stream and previews the contents of a stream by capturing a few seconds of streaming data.

We can also aggregate data with `dplyr`,

```{r}
stream_read_text(sc, "source/") %>%
  summarise(n = n())
```

and even join across many concurrent streams:

```{r}
left_join(
  stream_read_text(sc, "source/") %>% stream_watermark(),
  stream_read_text(sc, "source/") %>% stream_watermark() %>% mutate(random = rand()),
)
```

Notice that some operations, like joins, require watermarks to define late data and specify when to stop waiting for it. You can specify watermarks in `sparklyr` using `stream_watermak()`, see also [handling late data](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#handling-late-data-and-watermarking) in Spark's documentation.

### Pipelines {#streams-pipelines}

Spark pipelines can be used for scoring streams; however, not to train over streaming data. The former is fully supperted while the latter is a feature under acteve development by the Spark community.

To use a pipeline for scoring a stream, first train a Spark pipeline over a static dataset. Once trained, save the pipeline, then reload and score over a stream as follows:

```{r eval=FALSE}
fitted_pipeline <- ml_load(sc, "iris-fitted/")

stream_read_csv(sc, "iris-in") %>%
  sdf_transform(fitted_pipeline) %>%
  stream_write_csv("iris-out")
```

### R Code {#streams-r}

Arbitrary R code can also be used to transform a stream with the use of `spark_apply()`. Just like processing data over a Spark data frame, `spark_apply()` runs over each executor in the cluster where data is partitioned to parallelize over wide streams or reduce latency.

The following example splits a stream of `Row #` line entries and adds jitter using R code:

```{r}
stream_read_text(sc, "source/") %>%
  spark_apply(function(df) {
    data.frame(
      rows = jitter(
        as.numeric(
          gsub("Row ", "", df$text)
        )
      )
    )
  })
```

## Shiny

Streams can be used with Shiny by making use of the `reactiveSpark()` to retrieve the stream as a reactive data source. Internally, `reactiveSpark()` makes use of `reactivePoll()` to check the stream's timestamp and collect the stream contents when needed.

A simple Shiny application using a `reactiveSpark()` source looks like:

```{r eval = FALSE}
library(shiny)
library(sparklyr)
library(dplyr)

sc <- spark_connect(master = "local")

ui <- fluidPage(
  sidebarLayout(
    mainPanel(
      tableOutput("table")
    )
  )
)

server <- function(input, output, session) {
  pollData <- stream_read_text(sc, "source/") %>%
    summarise(n = n()) %>%
    reactiveSpark(session = session)

  output$table <- renderTable({
    pollData()
  })
}

shinyApp(ui = ui, server = server)
```

```{r echo=FALSE, message=FALSE}
spark_disconnect(sc)

unlink("source/", recursive = TRUE)
unlink("destination/", recursive = TRUE)
```


