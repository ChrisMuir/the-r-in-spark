---
title: "Walkthrough Excersises"
output: html_notebook
---

```{r}
library(sparklyr)
```

```{r}
spark_install()                                      # Install Apache Spark
sc <- spark_connect(master = "local")                # Connect to Spark cluster
```

```{r}
library(dplyr)
library(DBI)

dir.create("input")                                  # Create cars folder
write.csv(mtcars, "input/cars.csv")                  # Write data in R
```

```{r}
cars_tbl <- spark_read_csv(sc, "cars", "input/")     # Read data in Spark

summarize(cars_tbl, n = n())                             # Count records with dplyr
dbGetQuery(sc, "SELECT count(*) FROM cars")          # Count records with DBI
```

```{r}
ml_linear_regression(cars_tbl, mpg ~ wt + cyl)       # Perform linear regression

ml_pipeline(sc) %>%                                  # Define Spark pipeline
  ft_r_formula(mpg ~ wt + cyl) %>%                   # Add formula transformation
  ml_linear_regression()                             # Add model to pipeline
```

```{r}
spark_context(sc) %>% invoke("version")              # Extend sparklyr with Scala
```

```{r}
spark_apply(cars_tbl, nrow)                          # Extend sparklyr with R
```

```{r}
stream_read_csv(sc, "input/") %>%                    # Define Spark stream
  filter(mpg > 30) %>%                               # Add dplyr transformation
  stream_write_json("output/")                       # Start processing stream
```

From the terminal, you can simulate simulate the input stream by coping the original file multiple times using `cp cars.csv $RANDOM.csv`.

```{r}
spark_disconnect(sc)
```
